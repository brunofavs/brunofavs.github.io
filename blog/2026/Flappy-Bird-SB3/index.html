<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Teaching PPO Agent to Play Flappy Bird with SB3/Gymnasium | Bruno Silva </title> <meta name="author" content="Bruno Silva"> <meta name="description" content="In this post, I explored a toy example to learn Stable Baselines3 (SB3) and Gymnasium."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://brunofavs.github.io/blog/2026/Flappy-Bird-SB3/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?3902e296509171a054137ba0533749c2" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css" integrity="sha256-3qTIuuUWIFnnU3LpQMjqiXc0p09rvd0dmj+WkpQXSR8=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Bruno</span> Silva </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Teaching PPO Agent to Play Flappy Bird with SB3/Gymnasium</h1> <p class="post-meta"> Created on January 09, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/tooling"> <i class="fa-solid fa-hashtag fa-sm"></i> tooling,</a>   <a href="/blog/tag/implementation"> <i class="fa-solid fa-hashtag fa-sm"></i> implementation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/Flappy-Bird-SB3/flappy_cropped.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <h1 id="introduction">Introduction</h1> <p>Hello fellow reader!</p> <p>I am writing this post to report and document a little side project I worked on during Christmas to gain proficiency in two of the tools I will be using throughout my Ph.D., namely: <strong>Stable Baselines 3 (SB3)</strong> and <strong>Gymnasium (Gym)</strong>.</p> <p>My thought process was that since I need to learn many tools and frameworks to actually start working on <em>Contact-Rich HRM with RL</em>, I should try to isolate a few tools so that I don’t become overwhelmed. To do so, I chose to leave Robots aside for a bit, and lean on a much simpler environment with only 2 actions: the game <em>Flappy Bird</em> ! The video shown above is my final agent trained with <em>Proximal Policy Optimization (PPO)</em> at around 920k steps. The agent can achieve a score of 40, although it sometimes fails early for reasons I am not sure about.</p> <p>Although there are countless Gym environments available on the internet, I figured using a premade environment would make this project too easy. Additionally, I will not be using premade environments in the future, so I made the conscious choice to design my own environment. I took a working game with <em>Pygame</em> from a <a href="https://github.com/TimoWilken/flappy-bird-pygame" rel="external nofollow noopener" target="_blank">repository</a>, meant to be played by humans, and modified it into a fully working Gym environment.</p> <p>Finally, the choice of algorithm and low level RL stuff was not explored for simplicity’s sake. As such, I used <em>SB3</em>’s <em>PPO</em> implementation with a <em>Multi-Layer Perceptron (MLP)</em> policy. I chose <em>PPO</em> without much thought into it. I chose it over <em>Soft-Actor Critic (SAC)</em> because <em>SAC</em> is not compatible with discrete action spaces such as the one in this environment.</p> <p>In the rest of the post, I will elaborate various topics such as:</p> <ul> <li>Using custom environments;</li> <li>Observation and Action Spaces;</li> <li>Reward Shaping;</li> <li>Acknowledged issues and ways to make the problem more interesting.</li> </ul> <h1 id="using-custom-environments">Using Custom Environments</h1> <p>All environments in Gym must inherit from a parent class <code class="language-plaintext highlighter-rouge">gym.Env</code> and follow the following structure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">FlappyBird_v1</span><span class="p">(</span><span class="n">gym</span><span class="p">.</span><span class="n">Env</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Initialize positions - will be set randomly in reset()
</span>
        <span class="c1"># Define what the agent can observe, 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="nc">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Define what actions are available (0: do nothing, 1: flap)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="nc">Discrete</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># # Initialize other things that need to be initiliazed only once
</span>
        <span class="bp">...</span>

    <span class="k">def</span> <span class="nf">_get_obs</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Convert internal state to observation format.

        </span><span class="sh">"""</span>

        <span class="k">return</span> <span class="n">observations</span>

    <span class="k">def</span> <span class="nf">_get_info</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Compute auxiliary information for debugging.
        </span><span class="sh">"""</span>
        
        <span class="k">return</span> <span class="n">info</span>


    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">options</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Start a new episode.

        Reset the environment into the start state of an new episode.
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Reset state and return initial observation/info
</span>
        <span class="n">observation</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_obs</span><span class="p">()</span>
        <span class="n">info</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_info</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">observation</span><span class="p">,</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Execute one timestep within the environment.

        </span><span class="sh">"""</span>

        <span class="c1"># Compute one environment step
</span>
        <span class="c1"># Compute step reward
</span>
        <span class="c1"># Execute given action
</span>

        <span class="n">terminated</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">done</span>
        <span class="n">truncated</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="n">observation</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_obs</span><span class="p">()</span>
        <span class="n">info</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_info</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">observation</span><span class="p">,</span> <span class="n">cumulative_reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span>

</code></pre></div></div> <p>This structure consists of 4 mandatory methods and 1 auxiliary method:</p> <h3 id="__init__"><code class="language-plaintext highlighter-rouge">__init__</code></h3> <p>The <code class="language-plaintext highlighter-rouge">_init_</code> method includes all variables and mechanisms that need to be initialized only once. These include:</p> <ul> <li>Defining the environment observation space (<a href="https://gymnasium.farama.org/api/spaces/#fundamental-spaces" rel="external nofollow noopener" target="_blank">types of spaces</a>);</li> <li>Defining the environment action space (see URL above);</li> <li>Define other variables, environment logic and flows that need to be initialized only once: <ul> <li>Initialize Pygame.</li> </ul> </li> </ul> <p><strong>TODO: Add reference to observation and action spaces</strong></p> <p>Like any other <code class="language-plaintext highlighter-rouge">__init__</code> method of a Python Class, this method is only executed on class instance creation.</p> <h3 id="reset"><code class="language-plaintext highlighter-rouge">reset</code></h3> <p>The <code class="language-plaintext highlighter-rouge">reset</code> method resets the environment to an initial state, which is required before calling <code class="language-plaintext highlighter-rouge">step</code> in a new episode. It returns the first agent observation for an episode and information. Here…</p> <ul> <li>It resets the bird’s instance;</li> <li>Clears the list of existing pipe pairs;</li> <li>Compute environment observations;</li> <li>Resets frame clock, game score and done/terminated from a previous episode.</li> </ul> <h3 id="step"><code class="language-plaintext highlighter-rouge">step</code></h3> <p>The <code class="language-plaintext highlighter-rouge">step</code> method is where the major chunk of the environment logic resides. It updates the environment with a given action, returning the next agent observation, the reward for taking that action, whether the environment has terminated or truncated due to the latest action, and information from the environment about the step.</p> <p>In this environment, in very simple terms, <code class="language-plaintext highlighter-rouge">step</code> does the following:</p> <ul> <li>Check whether to add a new pipe pair to the environment;</li> <li>Executes given action (Flap or do nothing);</li> <li>Checks for collisions;</li> <li>Computes step cumulative reward;</li> <li>Computes environment observations;</li> <li>Updates game score.</li> </ul> <h3 id="_get_obs"><code class="language-plaintext highlighter-rouge">_get_obs</code></h3> <p>This helper method translates the environment’s internal state into the observation format. This keeps the code DRY (Don’t Repeat Yourself) and makes it easier to modify the observation format later.</p> <p><strong>TODO: Add reference to observation and action spaces</strong></p> <h3 id="_get_info"><code class="language-plaintext highlighter-rouge">_get_info</code></h3> <p>Sometimes some data is only available inside <code class="language-plaintext highlighter-rouge">Env.step()</code> (like individual reward components, action success/failure, etc.). In those cases, it is useful to record this information to debug or for visualization purposes. The <code class="language-plaintext highlighter-rouge">_get_info</code> helper method is executed at the end of both <code class="language-plaintext highlighter-rouge">step</code> and <code class="language-plaintext highlighter-rouge">reset</code> and can contain a miscellaneous collection of information deemed to be useful when debugging.</p> <h3 id="making-an-environment-compatible-with-gym">Making an environment compatible with Gym</h3> <p>With the proper structure of a Gym environment discussed, I can now explain how to make a pre-existing environment compatible with Gym. To start, one must take a step back and think about which parts of our environment execute only once, which are part of the main game logic loop, and which parts would need to be reset for a new episode to begin. After identifying these, the code needs to be split into the aforementioned methods <code class="language-plaintext highlighter-rouge">__init__</code>, <code class="language-plaintext highlighter-rouge">step</code> and <code class="language-plaintext highlighter-rouge">reset</code>, respectively. Due to the <strong>object-oriented programming (OOP)</strong> nature of Gym, environments need to be adapted to be class-friendly, in part due to the limited scopes of each of the aforementioned methods.</p> <h1 id="observation-and-action-spaces">Observation and Action Spaces</h1> <p>Every RL agent needs a set of observation and possible actions. The possible actions in this game are either to flap or to do nothing and let gravity bring the bird down. Therefore, the action space was defined as a discrete action space given by the set $\mathcal{A}=\{0,1\}$, with 1 corresponding to flapping and 0 corresponding to doing nothing. There was a slight caveat that the original environment did not account for, that is the possibility to spam flaps which would cancel each other out and make the bird effectively float. This was fixed by adding a delay corresponding to the flight duration in which the action <em>flap</em> is nullified. One big problem I found in the beginning was that the game rendered at 60FPS, which meant 60 actions each second. In early training, both actions are taken with similar probability, and in order to maintain an average stable $Y$, the agent needs to do nothing for about 95% of steps. This led to problems with always hitting the top border. There was some thought put into modifying the action space to either flap or rest for $n$ steps. In the end, I tackled this problem through reward shaping (which is covered below), but this alternative approach may also have led to success.</p> <p>Regarding observation spaces, there was the choice between full and partial observability. Full observability would imply giving all available state information to the agent. This option has a greater performance ceiling, but comes with a ton of extra noise which would make training difficult and require fancier image processing mechanisms such as convolutional autoencoders to reduce dimensionality and extract relevant features, thereby reducing unnecessary noise. Here full observability would entail giving the game window, as well as other internal variables. Since this is a relatively simpler environment, I chose to stick with partial observability and hand-craft the features. The list of features I settled on were the following:</p> <ul> <li>Bird’s vertical position (y-coordinate)</li> <li>Next pipe’s horizontal distance (how far away)</li> <li>Next pipe’s gap vertical position (top/bottom of opening)</li> <li>Bird’s distance from top/bottom of screen (helps avoid boundaries)</li> </ul> <p>This kept the agent simple and without any unnecessary information. The idea was to give only what the agent needed to rely on to make a decision. Constants like the gap or width of a pipe pair, bird’s hitbox, and bird’s x-coordinate are not needed as the agent can learn those internally.</p> <h1 id="reward-shaping">Reward Shaping</h1> <p>Perhaps the most interesting and challenging aspect of this project. Coming from <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf" rel="external nofollow noopener" target="_blank">theoretical literature</a>, I learned that giving intermediate rewards to an agent is not ideal as it incentivizes a certain policy we think is best, while there may be a better policy that we are not capable of seeing. In classical RL learning problems, such as the maze problem, rewards are typically only given at the goal state. In real problems, this often leads to such a sparse reward that agents simply cannot learn. As such, here is the reward design I chose:</p> <ul> <li>Small survival reward -&gt; $+0.01/step$ meaning $+6/s$;</li> <li>Big reward for clearing a pipe -&gt; $+300$;</li> <li>Big punishment for colliding with either pipe or top/bottom borders -&gt; $-500$;</li> <li>Exponential punishment for consecutive flaps -&gt; $2 + (n_{consecutiveFlaps}^2)*0.01$</li> </ul> <p>The punishment for consecutive flaps was a way of dealing with over-flapping in the beginning as I previously mentioned. One issue I had initially was a completely wrong scale that disregarded framerate. I was giving a survival reward of $2$, which meant $120/s$. This taught the agent that surviving was more important than clearing pipes, so the agent did not get very far. Another problem I had was finding a correct scale for the rewards. These numbers I came up with came from thin air. While it worked, I wonder what other ways there are to optimize reward scaling.</p> <h1 id="results---discussion">Results &amp; Discussion</h1> <p>Using Stable Baselines 3’s features, I saved TensorBoard logs every $10k$ steps. Here are both the mean reward and mean episode length across steps:</p> <figure> <picture> <img src="/assets/img/Flappy-Bird-SB3/reward.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Mean reward across training steps</figcaption> </figure> <figure> <picture> <img src="/assets/img/Flappy-Bird-SB3/ep_len.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Mean episode length across training steps</figcaption> </figure> <p>Both plots show how the agent learned well the environment. At around $700k$ and $1M$ steps there were some hiccups. For the demo I chose the model at $920k$ steps, as it seems to be the most stable. Nevertheless, there are still many improvements to be made. The agent achieves a score of $20-40$ every few runs but in between gets stuck in the first few pipes and makes blunt mistakes. I am not sure why after going through the first, the agent seems to be much more stable. Additionally, whenever the agent fails, it is very often due to bumping its head on the corner of the up pipe when exiting.</p> <p>On the technical side, on future experiments, I would like to implement a way to start training from other trained models rather than from scratch.</p> <p>This environment can be made harder in many ways, whether through smaller or variable gaps, varying or greater horizontal velocity, less spacing between pipes, etc. However, I think this is a good breakpoint to move on to other more interesting things, such as messing with Robots again.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Bruno Silva. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js" integrity="sha256-EXHg3x1K4oIWdyohPeKX2ZS++Wxt/FRPH7Nl01nat1o=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>