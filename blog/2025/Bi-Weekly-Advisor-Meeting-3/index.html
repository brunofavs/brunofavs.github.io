<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bi-Weekly Advisor Meeting 18th of December 2025 | Bruno Silva </title> <meta name="author" content="Bruno Silva"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-3/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css" integrity="sha256-3qTIuuUWIFnnU3LpQMjqiXc0p09rvd0dmj+WkpQXSR8=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Bruno</span> Silva </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Bi-Weekly Advisor Meeting 18th of December 2025</h1> <p class="post-meta"> Created on December 18, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/bi-weekly"> <i class="fa-solid fa-hashtag fa-sm"></i> bi-weekly</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="contextsummary">Context/Summary</h1> <p>On my previous meeting with my advising team, I was reminded that I was leaving on key thematic out of my state-of-the-art review, which will come in later in my work: <strong>Meta-Learning</strong>. So far I had been looked into:</p> <ul> <li>Contact-rich relating to Reinforcement Learning (<a href="/blog/2025/Bi-Weekly-Advisor-Meeting-1/">Post</a>);</li> <li>Contact-rich Human-Robot Collaboration (<a href="/blog/2025/Bi-Weekly-Advisor-Meeting-2/">Post</a>); <ul> <li>Intent comprehension through contact interaction (<a href="https://www.researchgate.net/publication/362027689_A_review_on_interaction_control_for_contact_robots_through_intent_detection" rel="external nofollow noopener" target="_blank">Paper</a>)</li> </ul> </li> </ul> <p>From here I was missing <em>Meta-Learning</em> and <em>Model-Based Reinforcement Learning</em>.</p> <p>Regarding the latter, I have not read articles focused solely on <em>Model-Based RL</em> with <em>Contact-rich HRC</em>. I have however stumbled across a few paragraphs in other papers discussing <strong>Human-Modelling</strong>, which will be something I will be elaborating further down in this post. Come to think of it, the human is the most complex and hard entity to model in the environments I am interested in.</p> <p>Regarding the former, these weeks have been a bumpy ride. <em>TLDR</em> is that I’m feeling big gaps in Meta-Learning theory to properly understand what I am reading. After tweaking the keywords, I settled on the following search equation:</p> <div class="language-vb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TITLE</span><span class="o">-</span><span class="n">ABS</span><span class="o">-</span><span class="n">KEY</span> <span class="p">(</span> <span class="s">"human-robot interaction"</span> <span class="n">OR</span> <span class="s">"human robot interaction"</span> <span class="n">OR</span> <span class="s">"human-robot collaborat*"</span> <span class="n">OR</span> <span class="s">"human-robot cooperat*"</span> <span class="n">OR</span> <span class="s">"physical human-robot interaction"</span> <span class="n">OR</span> <span class="s">"phri"</span> <span class="n">OR</span> <span class="s">"collaborative robot*"</span> <span class="n">OR</span> <span class="s">"cobot*"</span> <span class="n">OR</span> <span class="s">"hrc"</span> <span class="p">)</span>
<span class="n">AND</span> <span class="n">TITLE</span><span class="o">-</span><span class="n">ABS</span><span class="o">-</span><span class="n">KEY</span> <span class="p">(</span> <span class="s">"meta RL"</span> <span class="n">OR</span> <span class="s">"meta-RL"</span> <span class="n">OR</span> <span class="s">"meta reinforce*"</span> <span class="n">OR</span> <span class="s">"meta-reinforce*"</span> <span class="n">OR</span> <span class="s">"meta learning"</span> <span class="n">OR</span> <span class="s">"meta-learning"</span> <span class="n">OR</span> <span class="s">"metalearning"</span> <span class="n">OR</span> <span class="s">"meta-learn*"</span> <span class="p">)</span>
</code></pre></div></div> <p>… that yielded 30 articles, from which I saved 12 with interesting titles: (bold means thoroughly analised)</p> <ul> <li><strong>Adaptable automation with modular deep reinforcement learning and policy transfer</strong></li> <li><strong>Complementary learning-team machines to enlighten and exploit human expertise</strong></li> <li><strong>Active Exploration and Parameterized Reinforcement Learning Applied to a Simulated Human-Robot Interaction Task</strong></li> <li><strong>A behavioural transformer for effective collaboration between a robot and a non-stationary human</strong></li> <li><em>Meta-Learning-Based Optimal Control for Soft Robotic Manipulators to Interact with Unknown Environments</em></li> <li><em>Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in Human-Robot Interaction</em></li> <li><em>Meta-Learning for Text-Based Object Localization in Robotic Manipulation with DNN and CLIP</em></li> <li><em>Concept of an Intuitive Human-Robot-Collaboration via Motion Tracking and Augmented Reality</em></li> <li><em>Fast User Adaptation for Human Motion Prediction in Physical Human–Robot Interaction</em></li> <li><em>Developing the robotic space-force boundary of physical interaction perception in an infant way</em></li> </ul> <p>Considering there are not review papers on this matter yet, I plan to do a short review of these papers in this post, highlighting the details they share, and also where I struggled more.</p> <p>Generally, I found myself a bit lost reading these papers. I feel like I still lack key foundations in Meta-Learning to properly grasp what I am reading. Given this, I have been considering to diverge my attention for a short period of time to read at least the few chapters of <em>A Tutorial on Meta-Reinforcement Learning</em> (DOI 10.1561/2200000080). This tutorial was published earlier in April of this year, on <em>Foundations and Trends in Machine Learning</em>. It is co-authored by <em>Chelsea Finn</em>, one of the biggest names in <em>Meta-Learning</em>. Simultaneously, I’m hesitant about possibly investing time into something that I will work with only much later on. It is easy to fall in the rabbit hole, so if I choose this path I have to thread carefully to only gather the key aspects to help me understand the papers I am trying to review for the review paper I plan to publish next year.</p> <p>On an unrelated note, I found a <a href="https://www.reddit.com/r/reinforcementlearning/comments/1pjrnrn/if_youre_learning_rl_i_wrote_a_tutorial_about/" rel="external nofollow noopener" target="_blank">post</a> about a very interesting <a href="https://www.reinforcementlearningpath.com/step-by-step-soft-actor-critic-sac-implementation-in-sb3-with-pytorch/" rel="external nofollow noopener" target="_blank">SAC tutorial</a> using SB3 and PyTorch while browsing through Reddit on <a href="https://www.reddit.com/r/reinforcementlearning" rel="external nofollow noopener" target="_blank">r/reinforcementlearning</a>. I am planning on following this soon, I reckon I will sort of kill 2 birds with 1 stone, as I have been meaning to start learning SB3 for my future endeavors as well as SAC, one of the leading algorithms in Reinforcement Learning <em>SoTA</em>. On the previous meeting, we talked about <em>Isaac Lab</em>, a very powerful RL simulator for robotics from NVIDIA. Since then, I left the learning of this simulator on hold. I feel like I benefit more from learning SB3 and such right now, as I can not do much more other than create simulation environments without knowing these tools first.</p> <h2 id="short-critical-review">Short Critical Review</h2> <p>I was initially thinking about doing a meta analysis of the papers read, but I did not gather enough quantitative data to do so. Given this, I will do more of a simple critical review, narrating the key themes and insights. I only analyzed the abstracts, introductions, related works and conclusions. Details about the methods in the individual studies are not important right now. For now, I’m content to find general trends and topics that pop up frequently in Meta-Learning studies.</p> <h4 id="a-behavioural-transformer-for-effective-collaboration-between-a-robot--and-a-non-stationary-human">A behavioural transformer for effective collaboration between a robot and a non-stationary human</h4> <p>This paper discusses a topic with which I was previously unfamiliar, but which is highly relevant: <strong>Human Modeling</strong></p> <p>In human-robot collaboration scenarios, the human is typically considered part of the environment. This makes the environment time-dependent, due to the non-stationary human behavior. The human can not be an agent because the robot has no direct control over its actions. If an environment is non-stationary it means that either or both the state transition function and the reward function vary over time.</p> <p>One frequent approach to deal with non-stationary environments is the use of Multiagent RL. Multiagent RL: Multiagent reinforcement learning (MARL) aims to tackle non-stationarity. This approach is not feasible for human-robot collaboration settings as it is not possible to control the human agents.</p> <details> <summary><b>Side Note on how MARL tackles non-stationarity</b> Click to expand</summary> <p> Answer from <a href="https://chatgpt.com/s/t_6940d38c62ac81919b8733de98e6b24a" rel="external nofollow noopener" target="_blank">GPT</a>, modified. </p> <p> In standard (single-agent) reinforcement learning, the environment is usually assumed to be <strong>stationary</strong>: the transition dynamics and reward function do not change over time. This assumption is violated as soon as <strong>multiple learning agents</strong> are present. </p> <h3>Why non-stationarity arises</h3> <p> In a multi-agent setting, each agent is part of the environment from the perspective of the others. As agents <strong>learn and update their policies</strong>, their behavior changes over time. Consequently: </p> <ul> <li> The <strong>transition dynamics</strong> experienced by any given agent change. </li> <li> The <strong>reward distribution</strong> may also change. </li> <li> From a single agent’s viewpoint, the environment is no longer stationary or Markovian. The Markovian property of an environment is the core idea that the future depends only on the present, not the past; the next state and reward are conditionally independent of all previous states and actions, given only the current state and action. </li> </ul> <p> This phenomenon is known as <strong>environmental non-stationarity</strong>. </p> <h3>Why MARL “aims to tackle” non-stationarity</h3> <p> Multiagent reinforcement learning explicitly acknowledges and addresses this issue by designing methods that can cope with or mitigate non-stationarity, for example: </p> <ul> <li> <strong>Centralized training, decentralized execution (CTDE):</strong><br> During training, agents have access to joint observations or joint actions, making the learning problem closer to a stationary one. </li> <li> <strong>Opponent / agent modeling:</strong><br> Agents learn representations or models of other agents’ behaviors, allowing them to adapt to policy changes. </li> <li> <strong>Joint policy learning:</strong><br> Learning coordinated policies reduces uncontrolled policy drift. </li> <li> <strong>Game-theoretic formulations:</strong><br> Equilibrium concepts (e.g., Nash equilibria) provide stable solution targets despite learning dynamics. </li> </ul> <h3>Key intuition (one-sentence takeaway)</h3> MARL aims to tackle non-stationarity because, in multi-agent environments, each agent’s learning continuously changes the environment experienced by others, violating the stationarity assumptions of standard RL. </details> <p>Given that <em>MARL</em> is not an option, this study also discusses some ways to model human behavior, among which:</p> <ul> <li>Programming rules (the article did not give examples about this)</li> <li>Imitation learning</li> <li>Probabilistic models with or without Boltzmann rationality (agent is <em>boundedly rational</em>: it tends to prefer higher-value actions, but it may still choose lower-value ones with some probability.)</li> </ul> <p>From this citations in article I also saved some potentially interesting articles on human modelling for AI applications:</p> <ul> <li> <em>LESS is More: Rethinking Probabilistic Models of Human Behavior</em> (78 citations);</li> <li> <em>Optimal Behavior Prior: Data-Efficient Human Models for Improved Human-AI Collaboration</em> (17 citations);</li> <li> <em>Prediction of Human Behavior in Human–Robot Interaction Using Psychological Scales for Anxiety and Negative Attitudes Toward Robots</em> (588 citations)</li> </ul> <p>The article uses follows the <em>zero-shot Meta-Learning</em> paradigm. On the <em>Meta-Learning</em> tutorial aforementioned, there are chapters for both zero-shot and multi-shot Meta-Learning. As a general AI concept, I have the vague idea that a zero-shot approach is one where a model can understand a new sample never seen during training, relying on pre-learned semantic knowledge. This concept seems to overlap with the definition of <em>Meta-Learning</em>, but that is where my understanding ends. The search equation I mentioned in this article shows a <a href="https://www.researchgate.net/publication/369516237_Recent_advances_of_few-shot_learning_methods_and_applications" rel="external nofollow noopener" target="_blank">review article</a> about few-shot learning methods, but I failed to recognize before how it could relate to what I am studying. Might be worth a read.</p> <p>Like in other articles, a frequent environment used to test human models is the soccer pass. The agent has to learn the human pass behavior, more or less power, varying angles, etc.</p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_3/soccer.png" class="img-fluid rounded z-depth-1" width="800" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h4 id="active-exploration-and-parameterized-reinforcement--learning-applied-to-a-simulated-human-robot--interaction-task">Active exploration and parameterized reinforcement learning applied to a simulated human-robot interaction task</h4> <p>This study was too overly focused on multi-armed bandits. Multi-armed bandits are a typical type of problem used when one is first learning <em>Reinforcement Learning</em>. Whereas typical RL environments are modelled with a <em>Markov Decision Process</em> (MDP), involving state transition probability functions, discount factors, etc… bandits are a simpler environment with only one state. For example choosing a restaurant to eat for a series of days. The concern is shifted from which action on a certain state to only which action has a higher value.</p> <p>Like the previous study, this one also refers <em>Boltzmann Rationality</em> for modelling human actions as well as the 2D Soccer Pass environment. Although the latter only briefly.</p> <h4 id="adaptable-automation-with-modular-deep-reinforcement-learning-and-policy-transfer">Adaptable automation with modular deep reinforcement learning and policy transfer</h4> <p>This study talks about a lot of new (to me) ideas in modular Reinforcement Learning. In its core, Modular RL is the idea of decomposing a task into several simpler modules, which can be learned separately and reused in learning new manipulation tasks. Each module can have its own reward function.</p> <p>The general idea is built on the assumption that modules are reusable neural network functions. Therefore, they can be pre-trained and recombined in different ways to tackle new tasks. Thus, instead of training a single network on a complex task that would take a long time, one can train on many different task module networks.</p> <p>The paper proposes a new variation of the SAC algorithm, HASAC. Its goal is to enhance an agent’s adaptability to new tasks by transferring the learned policies of former tasks to the new task through a “hyper-actor”. I did not dive deeply into how this works, as I have still to properly study the baseline SAC algorithm.</p> <p>Uses the Meta World benchmark, from 2020, containing various single and multi-tasks robotic manipulation tasks.</p> <p>The study also highlights one concept that has a particular interest in Meta-Learning, inductive bias.</p> <p>From <a href="https://www.reddit.com/r/MLQuestions/comments/egof3l/comment/fc8u0fi/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button" rel="external nofollow noopener" target="_blank">here</a>:</p> <blockquote> <p>Inductive bias is restricting your hypothesis space to a certain class of hypotheses. In simpler terms, it’s a guess about what the function looks like, before looking at data. Take regression: if you have a finite set of points, you can draw infinitely many different functions that fit them. But if you assume that the function is linear, you remove almost all possibilities (there are still infinitely many linear functions, but way, way fewer than functions in general). Common inductive biases are smoothness and sparsity. Pretty much all used models include smoothness somehow: similar inputs have similar outputs. Sparsity is also very common: it’s the assumption that the function depends on few inputs. Without an inductive bias, machine learning is impossible. If something in machine learning works well, more often than not it’s because of its inductive bias. For example, convolutional neural networks are biased towards the kind of structure present in images: pixels next to each other are similar, there are similar features (e.g. eyes) present in different locations, big features are made of smaller features. A consequence of this is the no-free-lunch theorem. If your inductive bias works well on some type of data, it has to work badly on other kinds of data.</p> </blockquote> <p>I think this takes special importance in Meta-Learning because the assumptions we make about the tasks we are learning can influence what we expect new tasks to look like. This may or may not be ideal. I would guess it depends if we know which tasks the agent might be faced with.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Bruno Silva. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js" integrity="sha256-EXHg3x1K4oIWdyohPeKX2ZS++Wxt/FRPH7Nl01nat1o=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>