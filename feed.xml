<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://brunofavs.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://brunofavs.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-16T19:45:04+00:00</updated><id>https://brunofavs.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Bi-Weekly Advisor Meeting 18th of December 2025</title><link href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-3/" rel="alternate" type="text/html" title="Bi-Weekly Advisor Meeting 18th of December 2025"/><published>2025-12-18T15:12:00+00:00</published><updated>2025-12-18T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-3</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-3/"><![CDATA[<h1 id="contextsummary">Context/Summary</h1> <p>On my previous meeting with my advising team, I was reminded that I was leaving on key thematic out of my state-of-the-art review, which will come in later in my work: <strong>Meta-Learning</strong>. So far I had been looked into:</p> <ul> <li>Contact-rich relating to Reinforcement Learning (<a href="/blog/2025/Bi-Weekly-Advisor-Meeting-1/">Post</a>);</li> <li>Contact-rich Human-Robot Collaboration (<a href="/blog/2025/Bi-Weekly-Advisor-Meeting-2/">Post</a>); <ul> <li>Intent comprehension through contact interaction (<a href="https://www.researchgate.net/publication/362027689_A_review_on_interaction_control_for_contact_robots_through_intent_detection">Paper</a>)</li> </ul> </li> </ul> <p>From here I was missing <em>Meta-Learning</em> and <em>Model-Based Reinforcement Learning</em>.</p> <p>Regarding the latter, I have not read articles focused solely on <em>Model-Based RL</em> with <em>Contact-rich HRC</em>. I have however stumbled across a few paragraphs in other papers discussing <strong>Human-Modelling</strong>, which will be something I will be elaborating further down in this post. Come to think of it, the human is the most complex and hard entity to model in the environments I am interested in.</p> <p>Regarding the former, these weeks have been a bumpy ride. <em>TLDR</em> is that I’m feeling big gaps in Meta-Learning theory to properly understand what I am reading. After tweaking the keywords, I settled on the following search equation:</p> <div class="language-vb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TITLE</span><span class="o">-</span><span class="n">ABS</span><span class="o">-</span><span class="n">KEY</span> <span class="p">(</span> <span class="s">"human-robot interaction"</span> <span class="n">OR</span> <span class="s">"human robot interaction"</span> <span class="n">OR</span> <span class="s">"human-robot collaborat*"</span> <span class="n">OR</span> <span class="s">"human-robot cooperat*"</span> <span class="n">OR</span> <span class="s">"physical human-robot interaction"</span> <span class="n">OR</span> <span class="s">"phri"</span> <span class="n">OR</span> <span class="s">"collaborative robot*"</span> <span class="n">OR</span> <span class="s">"cobot*"</span> <span class="n">OR</span> <span class="s">"hrc"</span> <span class="p">)</span>
<span class="n">AND</span> <span class="n">TITLE</span><span class="o">-</span><span class="n">ABS</span><span class="o">-</span><span class="n">KEY</span> <span class="p">(</span> <span class="s">"meta RL"</span> <span class="n">OR</span> <span class="s">"meta-RL"</span> <span class="n">OR</span> <span class="s">"meta reinforce*"</span> <span class="n">OR</span> <span class="s">"meta-reinforce*"</span> <span class="n">OR</span> <span class="s">"meta learning"</span> <span class="n">OR</span> <span class="s">"meta-learning"</span> <span class="n">OR</span> <span class="s">"metalearning"</span> <span class="n">OR</span> <span class="s">"meta-learn*"</span> <span class="p">)</span>
</code></pre></div></div> <p>… that yielded 30 articles, from which I saved 12 with interesting titles: (bold means thoroughly analised)</p> <ul> <li><strong>Adaptable automation with modular deep reinforcement learning and policy transfer</strong></li> <li><strong>Complementary learning-team machines to enlighten and exploit human expertise</strong></li> <li><strong>Active Exploration and Parameterized Reinforcement Learning Applied to a Simulated Human-Robot Interaction Task</strong></li> <li><strong>A behavioural transformer for effective collaboration between a robot and a non-stationary human</strong></li> <li><em>Meta-Learning-Based Optimal Control for Soft Robotic Manipulators to Interact with Unknown Environments</em></li> <li><em>Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in Human-Robot Interaction</em></li> <li><em>Meta-Learning for Text-Based Object Localization in Robotic Manipulation with DNN and CLIP</em></li> <li><em>Concept of an Intuitive Human-Robot-Collaboration via Motion Tracking and Augmented Reality</em></li> <li><em>Fast User Adaptation for Human Motion Prediction in Physical Human–Robot Interaction</em></li> <li><em>Developing the robotic space-force boundary of physical interaction perception in an infant way</em></li> </ul> <p>Considering there are not review papers on this matter yet, I plan to do a short review of these papers in this post, highlighting the details they share, and also where I struggled more.</p> <p>Generally, I found myself a bit lost reading these papers. I feel like I still lack key foundations in Meta-Learning to properly grasp what I am reading. Given this, I have been considering to diverge my attention for a short period of time to read at least the few chapters of <em>A Tutorial on Meta-Reinforcement Learning</em> (DOI 10.1561/2200000080). This tutorial was published earlier in April of this year, on <em>Foundations and Trends in Machine Learning</em>. It is co-authored by <em>Chelsea Finn</em>, one of the biggest names in <em>Meta-Learning</em>. Simultaneously, I’m hesitant about possibly investing time into something that I will work with only much later on. It is easy to fall in the rabbit hole, so if I choose this path I have to thread carefully to only gather the key aspects to help me understand the papers I am trying to review for the review paper I plan to publish next year.</p> <p>On an unrelated note, I found a <a href="https://www.reddit.com/r/reinforcementlearning/comments/1pjrnrn/if_youre_learning_rl_i_wrote_a_tutorial_about/">post</a> about a very interesting <a href="https://www.reinforcementlearningpath.com/step-by-step-soft-actor-critic-sac-implementation-in-sb3-with-pytorch/">SAC tutorial</a> using SB3 and PyTorch while browsing through Reddit on <a href="https://www.reddit.com/r/reinforcementlearning">r/reinforcementlearning</a>. I am planning on following this soon, I reckon I will sort of kill 2 birds with 1 stone, as I have been meaning to start learning SB3 for my future endeavors as well as SAC, one of the leading algorithms in Reinforcement Learning <em>SoTA</em>. On the previous meeting, we talked about <em>Isaac Lab</em>, a very powerful RL simulator for robotics from NVIDIA. Since then, I left the learning of this simulator on hold. I feel like I benefit more from learning SB3 and such right now, as I can not do much more other than create simulation environments without knowing these tools first.</p> <h2 id="short-critical-review">Short Critical Review</h2> <p>I was initially thinking about doing a meta analysis of the papers read, but I did not gather enough quantitative data to do so. Given this, I will do more of a simple critical review, narrating the key themes and insights. I only analyzed the abstracts, introductions, related works and conclusions. Details about the methods in the individual studies are not important right now. For now, I’m content to find general trends and topics that pop up frequently in Meta-Learning studies.</p> <h4 id="a-behavioural-transformer-for-effective-collaboration-between-a-robot--and-a-non-stationary-human">A behavioural transformer for effective collaboration between a robot and a non-stationary human</h4> <p>This paper discusses a topic with which I was previously unfamiliar, but which is highly relevant: <strong>Human Modeling</strong></p> <p>In human-robot collaboration scenarios, the human is typically considered part of the environment. This makes the environment time-dependent, due to the non-stationary human behavior. The human can not be an agent because the robot has no direct control over its actions. If an environment is non-stationary it means that either or both the state transition function and the reward function vary over time.</p> <p>One frequent approach to deal with non-stationary environments is the use of Multiagent RL. Multiagent RL: Multiagent reinforcement learning (MARL) aims to tackle non-stationarity. This approach is not feasible for human-robot collaboration settings as it is not possible to control the human agents.</p> <details> <summary><b>Side Note on how MARL tackles non-stationarity</b> Click to expand</summary> <p> Answer from <a href="https://chatgpt.com/s/t_6940d38c62ac81919b8733de98e6b24a">GPT</a>, modified. </p> <p> In standard (single-agent) reinforcement learning, the environment is usually assumed to be <strong>stationary</strong>: the transition dynamics and reward function do not change over time. This assumption is violated as soon as <strong>multiple learning agents</strong> are present. </p> <h3>Why non-stationarity arises</h3> <p> In a multi-agent setting, each agent is part of the environment from the perspective of the others. As agents <strong>learn and update their policies</strong>, their behavior changes over time. Consequently: </p> <ul> <li> The <strong>transition dynamics</strong> experienced by any given agent change. </li> <li> The <strong>reward distribution</strong> may also change. </li> <li> From a single agent’s viewpoint, the environment is no longer stationary or Markovian. The Markovian property of an environment is the core idea that the future depends only on the present, not the past; the next state and reward are conditionally independent of all previous states and actions, given only the current state and action. </li> </ul> <p> This phenomenon is known as <strong>environmental non-stationarity</strong>. </p> <h3>Why MARL “aims to tackle” non-stationarity</h3> <p> Multiagent reinforcement learning explicitly acknowledges and addresses this issue by designing methods that can cope with or mitigate non-stationarity, for example: </p> <ul> <li> <strong>Centralized training, decentralized execution (CTDE):</strong><br/> During training, agents have access to joint observations or joint actions, making the learning problem closer to a stationary one. </li> <li> <strong>Opponent / agent modeling:</strong><br/> Agents learn representations or models of other agents’ behaviors, allowing them to adapt to policy changes. </li> <li> <strong>Joint policy learning:</strong><br/> Learning coordinated policies reduces uncontrolled policy drift. </li> <li> <strong>Game-theoretic formulations:</strong><br/> Equilibrium concepts (e.g., Nash equilibria) provide stable solution targets despite learning dynamics. </li> </ul> <h3>Key intuition (one-sentence takeaway)</h3> MARL aims to tackle non-stationarity because, in multi-agent environments, each agent’s learning continuously changes the environment experienced by others, violating the stationarity assumptions of standard RL. </details> <p>Given that <em>MARL</em> is not an option, this study also discusses some ways to model human behavior, among which:</p> <ul> <li>Programming rules (the article did not give examples about this)</li> <li>Imitation learning</li> <li>Probabilistic models with or without Boltzmann rationality (agent is <em>boundedly rational</em>: it tends to prefer higher-value actions, but it may still choose lower-value ones with some probability.)</li> </ul> <p>From this citations in article I also saved some potentially interesting articles on human modelling for AI applications:</p> <ul> <li><em>LESS is More: Rethinking Probabilistic Models of Human Behavior</em> (78 citations);</li> <li><em>Optimal Behavior Prior: Data-Efficient Human Models for Improved Human-AI Collaboration</em> (17 citations);</li> <li><em>Prediction of Human Behavior in Human–Robot Interaction Using Psychological Scales for Anxiety and Negative Attitudes Toward Robots</em> (588 citations)</li> </ul> <p>The article uses follows the <em>zero-shot Meta-Learning</em> paradigm. On the <em>Meta-Learning</em> tutorial aforementioned, there are chapters for both zero-shot and multi-shot Meta-Learning. As a general AI concept, I have the vague idea that a zero-shot approach is one where a model can understand a new sample never seen during training, relying on pre-learned semantic knowledge. This concept seems to overlap with the definition of <em>Meta-Learning</em>, but that is where my understanding ends. The search equation I mentioned in this article shows a <a href="https://www.researchgate.net/publication/369516237_Recent_advances_of_few-shot_learning_methods_and_applications">review article</a> about few-shot learning methods, but I failed to recognize before how it could relate to what I am studying. Might be worth a read.</p> <p>Like in other articles, a frequent environment used to test human models is the soccer pass. The agent has to learn the human pass behavior, more or less power, varying angles, etc.</p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_3/soccer.png" class="img-fluid rounded z-depth-1" width="800" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="active-exploration-and-parameterized-reinforcement--learning-applied-to-a-simulated-human-robot--interaction-task">Active exploration and parameterized reinforcement learning applied to a simulated human-robot interaction task</h4> <p>This study was too overly focused on multi-armed bandits. Multi-armed bandits are a typical type of problem used when one is first learning <em>Reinforcement Learning</em>. Whereas typical RL environments are modelled with a <em>Markov Decision Process</em> (MDP), involving state transition probability functions, discount factors, etc… bandits are a simpler environment with only one state. For example choosing a restaurant to eat for a series of days. The concern is shifted from which action on a certain state to only which action has a higher value.</p> <p>Like the previous study, this one also refers <em>Boltzmann Rationality</em> for modelling human actions as well as the 2D Soccer Pass environment. Although the latter only briefly.</p> <h4 id="adaptable-automation-with-modular-deep-reinforcement-learning-and-policy-transfer">Adaptable automation with modular deep reinforcement learning and policy transfer</h4> <p>This study talks about a lot of new (to me) ideas in modular Reinforcement Learning. In its core, Modular RL is the idea of decomposing a task into several simpler modules, which can be learned separately and reused in learning new manipulation tasks. Each module can have its own reward function.</p> <p>The general idea is built on the assumption that modules are reusable neural network functions. Therefore, they can be pre-trained and recombined in different ways to tackle new tasks. Thus, instead of training a single network on a complex task that would take a long time, one can train on many different task module networks.</p> <p>The paper proposes a new variation of the SAC algorithm, HASAC. Its goal is to enhance an agent’s adaptability to new tasks by transferring the learned policies of former tasks to the new task through a “hyper-actor”. I did not dive deeply into how this works, as I have still to properly study the baseline SAC algorithm.</p> <p>Uses the Meta World benchmark, from 2020, containing various single and multi-tasks robotic manipulation tasks.</p> <p>The study also highlights one concept that has a particular interest in Meta-Learning, inductive bias.</p> <p>From <a href="https://www.reddit.com/r/MLQuestions/comments/egof3l/comment/fc8u0fi/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">here</a>:</p> <blockquote> <p>Inductive bias is restricting your hypothesis space to a certain class of hypotheses. In simpler terms, it’s a guess about what the function looks like, before looking at data. Take regression: if you have a finite set of points, you can draw infinitely many different functions that fit them. But if you assume that the function is linear, you remove almost all possibilities (there are still infinitely many linear functions, but way, way fewer than functions in general). Common inductive biases are smoothness and sparsity. Pretty much all used models include smoothness somehow: similar inputs have similar outputs. Sparsity is also very common: it’s the assumption that the function depends on few inputs. Without an inductive bias, machine learning is impossible. If something in machine learning works well, more often than not it’s because of its inductive bias. For example, convolutional neural networks are biased towards the kind of structure present in images: pixels next to each other are similar, there are similar features (e.g. eyes) present in different locations, big features are made of smaller features. A consequence of this is the no-free-lunch theorem. If your inductive bias works well on some type of data, it has to work badly on other kinds of data.</p> </blockquote> <p>I think this takes special importance in Meta-Learning because the assumptions we make about the tasks we are learning can influence what we expect new tasks to look like. This may or may not be ideal. I would guess it depends if we know which tasks the agent might be faced with.</p>]]></content><author><name></name></author><category term="bi-weekly"/><summary type="html"><![CDATA[Context/Summary]]></summary></entry><entry><title type="html">Bi-Weekly Advisor Meeting 27 of November 2025</title><link href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2/" rel="alternate" type="text/html" title="Bi-Weekly Advisor Meeting 27 of November 2025"/><published>2025-11-27T15:12:00+00:00</published><updated>2025-11-27T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2/"><![CDATA[<h1 id="context">Context</h1> <ul> <li>Finished RL course <ul> <li>Use of Neural Networks for feature construction</li> <li>Policy Gradient Algorithms <ul> <li>Actor-Critic Methods</li> <li>Gaussian Policies for Continuous Action Spaces</li> </ul> </li> </ul> </li> <li>Search for papers combining HRC &amp; Contact-rich &amp; RL <ul> <li>No review papers found</li> <li>Broadening to non-review articles showed 124 articles under SE</li> <li>Should look for HRC &amp; Contact-Rich only, leaving RL out of it, maybe I’ll find a decent review paper this way</li> <li>Analysis of <em>Reinforcement Learning Based Variable Impedance Control for High Precision Human-robot Collaboration Tasks</em></li> </ul> </li> <li>Setting up containerized Isaac Lab <ul> <li>Settled on using Distrobox for ease of use and integration with host OS. Discarded Docker as I am not looking for isolation/sandboxing, I just need to use a software compatible with Ubuntu 22.04 (tested in Arch, launched but yielded many errors).</li> <li>Set up <a href="https://github.com/brunofavs/dotfiles/tree/main/scripts/Scripts/installation_scripts">installation scripts</a> to get all my dev tools, Isaac and Cuda working with one command. This will be useful whenever I need to work on one of the lab’s server. And also for the sake of reproducibility if I ever need to reinstall. Or even if somebody else in the lab wants to try it out.</li> <li>Isaac requires Ubuntu 22.04 and Python 3.11. To ensure Python compatibility I used <code class="language-plaintext highlighter-rouge">pyenv</code> with the <code class="language-plaintext highlighter-rouge">pyenv-virtualenv</code> plugin. Up until now I just used <code class="language-plaintext highlighter-rouge">virtualenvwrapper</code>, but this tool does not allow the creation of virtual environments with different Python versions.</li> </ul> </li> </ul> <h1 id="reinforcement-learning-based-variable-impedance-control-for-high-precision-human-robot-collaboration-tasks"><strong>Reinforcement Learning Based Variable Impedance Control for High Precision Human-robot Collaboration Tasks</strong></h1> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_2/paper_img1.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="descriptive-analysis">Descriptive Analysis</h3> <p>This paper proposed an impedance control method for high precision collaborative assembly tasks. The case study involved fitting shafts through cuboids with holes. Classic Peg-in-Hole. The shaft-hole clearance was 0.1 mm. The authors experimented both with 1 or 2 holes/shafts. The cuboid initial position was fixed and the shafts’ initial positions varied within a bounded range.</p> <p>In the first phase, using a depth camera, the robot locates and grabs the cuboid, and then moves along a reference trajectory without the human operator’s intervention. In the second phase, the human operator needs to correct the possible position deviation. During this time, the robot follows the human operator’s intention and senses the human force to change the end effector’s position.</p> <p>This work used PPO to determine the impedance parameters <em>M</em>, <em>B</em>, <em>K</em> (inertia, damping, and stiffness). After, these parameters were fed to the robots’ controller, which uses the following differential equation to compute the change in position.</p> \[M_t \Delta \ddot{x} + B_t \Delta \dot{x} + K_t \Delta x = \Delta f\] <p>The reward signal was the sum of the distance between the two end points of the object and the two target points.</p> <p>In simulation, the human force was replicated by a motor applying a force proportional to the distance from the target position. The authors argued that this approach has similar reasoning in comparison to the forces applied by a human. A human would apply a force proportional to how far off the shaft was from the hole.</p> <p>The authors reported that due to the limitations of the physics engine in the simulation environment, the cuboid may get stuck due to collision detection. The simulator used was MuJoCo.</p> <h3 id="key-takeaways-for-my-work">Key Takeaways for My Work</h3> <p>While this article presents interesting information, it diverges from my work in a few ways and has some notable gaps:</p> <ul> <li>I’m not so sure that this work is not a pure <em>contact-rich</em> task <em>per se</em>. It covers pre-contact, which I’m not interested in. It is not so clear through the paper how they get to the change in force used for in the aforementioned equation. I reckon they are just using the net force applied on the end effector. They do not seem to be taking the contact data to make the agent smarter. The human is the master and the robot is the slave, following blindly what the human suggests. The RL agent “simply” learns the impedance parameters so that the approach would still work for many humans applying different forces.</li> <li>While they show a physical set-up in the beginning to draw readers in, they later reveal that they did not leave simulation.</li> <li>The paper regards more the precision of the collaboration, whereas I would like my research to steer more into allowing a single agent to collaborate in multiple tasks, hence the later objective of implementing Meta RL.</li> </ul> <p>The papers highlight some points that are indeed relevant though:</p> <ul> <li>Differences in the skills of various operators bring difficulties to collaborative robots. This raises an extremely important question. Should I tailor the agent I will build to be sort of <strong>my</strong> little metal friend, designed to collaborate efficiently <strong>WITH ME</strong>, or be designed to collaborate with anyone? I believe both options are valid, and the former is definitely easier to train and has potential for more interesting results, which can be eye-catchy in an article.</li> <li>Common collaboration tasks are collaborative assembly and collaborative transportation, with a special emphasis on the latter with heavier objects.</li> </ul>]]></content><author><name></name></author><category term="bi-weekly"/><summary type="html"><![CDATA[Context]]></summary></entry><entry><title type="html">Why does the Policy Improvement Theorem not hold true in function approximation</title><link href="https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx/" rel="alternate" type="text/html" title="Why does the Policy Improvement Theorem not hold true in function approximation"/><published>2025-11-21T15:12:00+00:00</published><updated>2025-11-21T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx/"><![CDATA[<p><em>Page 254 Sutton</em></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"*The root cause of the difficulties with the discounted control setting is that with
function approximation we have lost the policy improvement theorem (Section 4.2). It is
no longer true that if we change the policy to improve the discounted value of one state
then we are guaranteed to have improved the overall policy in any useful sense. That
guarantee was key to the theory of our reinforcement learning control methods. With
function approximation we have lost it!*"
</code></pre></div></div> <p><a href="https://ai.stackexchange.com/questions/45071/why-policy-improvement-theorem-cant-be-applied-in-case-of-function-approximatio">Stack Exchange Useful Link</a></p> <p>Equation (4.7) from chapter <em>4.2</em> states that given $\pi$ and $\pi’$ as any pair of deterministic policies such that, for all $s \in \mathcal{S}$ :</p> \[q_{\pi}(s,\pi'(s)) \ge v_{\pi}(s)\] <p>Must mean that $\pi’$ is a strict improvement over $\pi$. A particular case occurs when all state values are equal but one. This was the basis of many tabular control methods. Tabular controls methods can improve individual states, so they allow <em>PIT</em> to be applied.</p> <h3 id="when-using-function-approximation">When using function approximation…</h3> <p>… any update in the weights modifies all states. Which means some get better, others get worse. It is a trade-off. Hence, the name function <strong>approximation</strong>. If we cannot assure that all state values are <em>at least</em> equal, the policy improvement theorem does not apply. Note that <em>PIT</em> also requires policies to be deterministic. Policies learned by Function approximation can both be deterministic or stochastic, so this particular condition can still be applied.</p> <h1 id="solution">Solution</h1> <p>Later in the same page…</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>*In Chapter 13 we introduce an
alternative class of reinforcement learning algorithms based on parameterized policies,
and there we have a theoretical guarantee called the “policy-gradient theorem” which
plays a similar role as the policy improvement theorem.*
</code></pre></div></div> <p>This is the class of algorithms I’m eager to learn about. These include all the Actor-Critic methods such as <strong>SAC</strong>. SAC is a state of the art algorithm used extensively in Robotics-RL research!</p>]]></content><author><name></name></author><category term="theory"/><summary type="html"><![CDATA[Page 254 Sutton]]></summary></entry><entry><title type="html">Bi-Weekly Advisor Meeting 06 of November 2025</title><link href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1/" rel="alternate" type="text/html" title="Bi-Weekly Advisor Meeting 06 of November 2025"/><published>2025-11-06T15:12:00+00:00</published><updated>2025-11-06T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1/"><![CDATA[<h1 id="context">Context</h1> <p>Last meeting, on the 24th of October, we concluded that it would be better to set bi-weekly deadlines to ensure that I didn’t wander off too much in the vast world of RL and Robotics and could ensure progress on my PhD problem. With this in mind, we agreed that it would be good to analyze some review papers on the fields of:</p> <ul> <li>Contact-Rich Robotics;</li> <li>Deep Reinforcement Learning;</li> <li>Model-based Reinforcement Learning.</li> </ul> <p>I began my search on Scopus with the following search equation: <code class="language-plaintext highlighter-rouge">TITLE ( "Robo*" ) AND ( TITLE ( "contact" ) OR TITLE-ABS-KEY ( "touch" ) ) AND PUBYEAR &gt; 2022 AND PUBYEAR &lt; 2026 AND ( LIMIT-TO ( DOCTYPE , "re" ) )</code></p> <p>Which yielded 34 results. Amongst those, a particular one caught my eye, named <strong>“A review on reinforcement learning for contact-rich robotic manipulation tasks”</strong> (<a href="https://doi.org/10.1016/j.rcim.2022.102517"><strong>DOI</strong></a>). This paper has 159 citations according to Google Scholar and was published on <em>Robotics and Computer-Integrated Manufacturing</em> (IF:11.4).</p> <details> <summary><b>PDF</b> Click to expand</summary> <object data="/assets/pdf/Elguea-Aguinaco_et_al._-_2023_-_A_review_on_reinforcement_learning_for_contact-rich_robotic_manipulation_tasks.pdf" width="1000" height="1000" type="application/pdf"></object> </details> <p>In sum, the paper touched on relevant topics such as:</p> <ul> <li>Rigid vs Deformable Object Manipulation;</li> <li>Tasks usually performed for both types of objects;</li> <li>Reinforcement Learning Algorithms most commonly used;</li> </ul> <h1 id="paper-analysis">Paper Analysis</h1> <h2 id="general-points">General Points</h2> <p><br/> Throughout the paper, an emphasis was put on contact stability. Many works dealt with issue due to contact stability.</p> <p>About 35% of the papers reviewed rely on human demonstration-assisted learning.</p> <h2 id="rigid-object-manipulation">Rigid Object Manipulation</h2> <p><br/></p> <p>The most common task covered by the papers reviewed was <em>Assembly and Insertion tasks</em>. <strong>Peg in Hole</strong> tasks are the most common benchmark as the problems that arise with this task are commonly found in assemblies. Performance, sample-efficiency and generalization capabilities are often the 3 desired goals. When it comes to performance, studies measure their performance targeted towards different purposes. Amongst others, precision, stability, safety, execution time or robustness.</p> <p>Safety wise, authors constrained the robot’s displacement and velocity motion commands, as well as the contact forces on the end-effector. In RL terms, the action space was constrained, which trades safety for prohibiting exploration leading to potentially better policies.</p> <p>Long training periods was also one of the reported issues. This is a recurrent drawback in high-dimensional tasks where large exploration is required. Therefore, many studies also put their efforts into pursuing greater sample efficiency. Many studies opted to find ways to give initial knowledge to the agents through various methods. For example: Human demonstrations; gathering knowledge from assembly tasks to use in disassembly tasks; combining RL with Fuzzy Logic using more hand-engineered rewards, etc.</p> <p>On the other end, the pursuit of sample efficiency generally leads to worse generalization capabilities, which in turn also worsens the agent transfer from simulation to reality. In this sense, a widely used approach in multiple studies is domain randomization. This technique allows to uniformly randomize a distribution of real data in predefined ranges in each training episode to obtain more robust policies. Two broad methods of domain randomization may be distinguished: visual randomization and dynamic randomization. Visual randomization targets to provide sufficient simulated variability of visual parameters, such as object location and their estimation in space or illumination. Dynamic randomization can help to acquire a robust control policy by randomizing various physical parameters in the simulator such as object dimensions, friction coefficients, or the damping coefficients of the robot joints, among others.</p> <p>Other studies in the pursuit of sample efficiency and generalization focus their attention on reward shaping and using models. There are studies using inverse RL from human demonstrations to learn a reward function. More recent studies combined dynamic movement primitives (DMPs) with RL to improve generalization. DMPs allow the contact-rich task to be divided into low-dimensional, easily adaptable sub-tasks, which also increase training efficiency. Others for instance, used meta-RL. Meta-learning enhances sample efficiency, as it enables explicit learning of a latent structure over a family of tasks that can later be easily adapted to the target task.</p> <p>In addition to assembly tasks, these tasks and associated challenges are also explored in studies:</p> <ul> <li>Disassembly; <ul> <li>High variability between items;</li> </ul> </li> <li>Polishing and Grinding; <ul> <li>Contact-stability is imperative;</li> </ul> </li> <li>Stacking and Unstacking; <ul> <li>Different from Pick &amp; Place because there is object to object contact;</li> </ul> </li> <li>Door/Drawer Opening; <ul> <li>Finding the correct axis of movement;</li> </ul> </li> <li>Pushing; <ul> <li>Unknown surface friction properties;</li> </ul> </li> <li>Mixing/Scooping;</li> <li><strong>Multiple tasks</strong> <ul> <li>High need for generalization.</li> </ul> </li> </ul> <h2 id="deformable-object-manipulation">Deformable Object Manipulation</h2> <p><br/></p> <p>Many of the problems presented before still hold here. For the moment I don’t plan on working with deformable objects so I will not elaborate further here. Nonetheless. there are the most frequent tasks:</p> <ul> <li>Rope Folding;</li> <li>Fabrics Folding;</li> <li>Tissue tensioning;</li> <li>Tissue cutting;</li> <li>Peg in hole (with one of parts being deformable).</li> </ul> <p>Here lies a distribution of tasks covered by studies for both Rigid and Deformable objects: </p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/5.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="comparison-of-algorithms-used">Comparison of algorithms used</h1> <p><br/></p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/6.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="safety-strategies-used">Safety strategies used</h1> <p><br/></p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/7.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="overview-of-studies-in-rigid-object-manipulation">Overview of studies in Rigid Object Manipulation</h1> <p><br/></p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/1.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/2.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/3.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="bi-weekly"/><summary type="html"><![CDATA[Context]]></summary></entry></feed>