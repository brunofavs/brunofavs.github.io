<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://brunofavs.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://brunofavs.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-09T17:59:23+00:00</updated><id>https://brunofavs.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Teaching PPO Agent to Play Flappy Bird with SB3/Gymnasium</title><link href="https://brunofavs.github.io/blog/2026/Flappy-Bird-SB3/" rel="alternate" type="text/html" title="Teaching PPO Agent to Play Flappy Bird with SB3/Gymnasium"/><published>2026-01-09T15:12:00+00:00</published><updated>2026-01-09T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2026/Flappy-Bird-SB3</id><content type="html" xml:base="https://brunofavs.github.io/blog/2026/Flappy-Bird-SB3/"><![CDATA[<div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/Flappy-Bird-SB3/flappy_cropped.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""/> </figure> </div> <h1 id="introduction">Introduction</h1> <p>Hello fellow reader!</p> <p>I am writing this post to report and document a little side project I worked on during Christmas to gain proficiency in two of the tools I will be using throughout my Ph.D., namely: <strong>Stable Baselines 3 (SB3)</strong> and <strong>Gymnasium (Gym)</strong>.</p> <p>My thought process was due to the fact that need to learn many tools and frameworks to actually start working on <em>Contact-Rich HRM with RL</em>, I should try to isolate a few tools so that I don’t become overwhelmed. To do so, I chose to leave Robots aside for a bit, and lean on a much simpler environment with only 2 actions: the game <em>Flappy Bird</em> ! The video shown above is my final agent trained with <em>Proximal Policy Optimization (PPO)</em> at around 920k steps. The agent can achieve a score of 40, although it is prone to seldom fail early for reasons about I am not sure.</p> <p>Although there are countless Gym environments throughout the internet, I figured using a premade environment would make this project too easy. Additionally, I will not ever be using premade environments in the future, so I made the conscious choice to design my own environment. I took a working game with <em>Pygame</em> from a <a href="https://github.com/TimoWilken/flappy-bird-pygame">repository</a>, meant to be played by humans, and modified it into a fully working Gym environment.</p> <p>Finally, the choice of algorithm and low level RL stuff was not explored for simplicity’s sake. As such, I used <em>SB3</em>’s <em>PPO</em> implementation with a <em>Multi-Layer Perceptron (MLP)</em> policy. I chose <em>PPO</em> without much thought into it. I chose it over <em>Soft-Actor Critic (SAC)</em> because <em>SAC</em> is not compatible with discrete action spaces such as the one in this environment.</p> <p>In the rest of the post, I will elaborate various topics such as:</p> <ul> <li>Using custom environments;</li> <li>Observation and Action Spaces;</li> <li>Reward Shaping;</li> <li>Acknowledged issues and ways to make the problem more interesting.</li> </ul> <h1 id="using-custom-environments">Using Custom Environments</h1> <p>All environments in Gym must inherit from a parent class <code class="language-plaintext highlighter-rouge">gym.Env</code> and follow the following structure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">FlappyBird_v1</span><span class="p">(</span><span class="n">gym</span><span class="p">.</span><span class="n">Env</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Initialize positions - will be set randomly in reset()
</span>
        <span class="c1"># Define what the agent can observe, 
</span>        <span class="n">self</span><span class="p">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="nc">Box</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

        <span class="c1"># Define what actions are available (0: do nothing, 1: flap)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="nc">Discrete</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># # Initialize other things that need to be initiliazed only once
</span>
        <span class="bp">...</span>

    <span class="k">def</span> <span class="nf">_get_obs</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Convert internal state to observation format.

        </span><span class="sh">"""</span>

        <span class="k">return</span> <span class="n">observations</span>

    <span class="k">def</span> <span class="nf">_get_info</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Compute auxiliary information for debugging.
        </span><span class="sh">"""</span>
        
        <span class="k">return</span> <span class="n">info</span>


    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">options</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Start a new episode.

        Reset the environment into the start state of an new episode.
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Reset state and return initial observation/info
</span>
        <span class="n">observation</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_obs</span><span class="p">()</span>
        <span class="n">info</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_info</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">observation</span><span class="p">,</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Execute one timestep within the environment.

        </span><span class="sh">"""</span>

        <span class="c1"># Compute one environment step
</span>
        <span class="c1"># Compute step reward
</span>
        <span class="c1"># Execute given action
</span>

        <span class="n">terminated</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">done</span>
        <span class="n">truncated</span> <span class="o">=</span> <span class="bp">False</span>

        <span class="n">observation</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_obs</span><span class="p">()</span>
        <span class="n">info</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_info</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">observation</span><span class="p">,</span> <span class="n">cumulative_reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span>

</code></pre></div></div> <p>This structure consists of 4 mandatory methods and 1 auxiliary method:</p> <h3 id="__init__"><code class="language-plaintext highlighter-rouge">__init__</code></h3> <p>The <code class="language-plaintext highlighter-rouge">_init_</code> method includes all variables and mechanisms that need to be initialized only once. These include:</p> <ul> <li>Defining the environment observation space (<a href="https://gymnasium.farama.org/api/spaces/#fundamental-spaces">types of spaces</a>);</li> <li>Defining the environment action space (see URL above);</li> <li>Define other variables, environment logics and flows that need to be only initialized once: <ul> <li>Initialize Pygame.</li> </ul> </li> </ul> <p><strong>TODO: Add reference to observation and action spaces</strong></p> <p>Like any other <code class="language-plaintext highlighter-rouge">__init__</code> method of a Python Class, this method is only executed on class instance creation.</p> <h3 id="reset"><code class="language-plaintext highlighter-rouge">reset</code></h3> <p>The <code class="language-plaintext highlighter-rouge">reset</code> method resets the environment to an initial state, which is required before calling <code class="language-plaintext highlighter-rouge">step</code> in a new episode. Returns the first agent observation for an episode and information. Here…</p> <ul> <li>It resets the bird’s instance;</li> <li>Clears the list of existing pipe pairs;</li> <li>Compute environment observations;</li> <li>Resets frame clock, game score and done/terminated from a previous episode.</li> </ul> <h3 id="step"><code class="language-plaintext highlighter-rouge">step</code></h3> <p>The <code class="language-plaintext highlighter-rouge">step</code> method is where the major chunk of the environment logic resides. Updates the environment with a given action, returning the next agent observation, the reward for taking that actions, if the environment has terminated or truncated due to the latest action and information from the environment about the step.</p> <p>In this environment, in very simple terms, <code class="language-plaintext highlighter-rouge">step</code> does the following:</p> <ul> <li>Check whether to add a new pipe pair to the environment;</li> <li>Executes given action (Flap or do nothing);</li> <li>Checks for collisions;</li> <li>Computes step cumulative reward;</li> <li>Computes environment observations;</li> <li>Updates game score.</li> </ul> <h3 id="_get_obs"><code class="language-plaintext highlighter-rouge">_get_obs</code></h3> <p>This helper method translates the environment’s internal state into the observation format. This keeps the code DRY (Don’t Repeat Yourself) and makes it easier to modify the observation format later.</p> <p><strong>TODO: Add reference to observation and action spaces</strong></p> <h3 id="_get_info"><code class="language-plaintext highlighter-rouge">_get_info</code></h3> <p>Sometimes some data is only available inside <code class="language-plaintext highlighter-rouge">Env.step()</code> (like individual reward components, action success/failure, etc.). In those cases, is it useful to record this intel to debug or for visualization purposes. The <code class="language-plaintext highlighter-rouge">_get_info</code> helper method is executed at the end of both <code class="language-plaintext highlighter-rouge">step</code> and <code class="language-plaintext highlighter-rouge">reset</code> and can contain a miscellaneous collection of information deemed to be useful when debugging.</p> <h3 id="making-an-environment-compatible-with-gym">Making an environment compatible with Gym</h3> <p>With the proper structure of a Gym environment discussed, I can now explain how to make a pre-existing environment compatible with Gym. To start, one must take a step back and think which parts of our environment execute only once, which are part of the main game logic loop, and which parts would need to be reset for a new episode to begin. After idealizing these, the code needs to be split into the aforementioned methods <code class="language-plaintext highlighter-rouge">__init__</code>,<code class="language-plaintext highlighter-rouge">step</code> and <code class="language-plaintext highlighter-rouge">reset</code>, respectively. Due to the <strong>object-oriented programming (OOP)</strong> of Gym, environments need to be adapted to be class-friendly, in some measure due to the limited scopes of each of the aforementioned methods.</p> <h1 id="observation-and-action-spaces">Observation and Action Spaces</h1> <p>Every RL agent needs a set of observation and possible actions. The possible actions in this game are either to flap or to do nothing and let gravity bring the bird down. Therefore, the action space was defined as a discrete action space given by a set $\mathcal{A}=\{0,1\}$, with 1 corresponding to flapping and 0 corresponding to doing nothing. There was a slight caveat that the original environment did not account for, that is the possibility to spam flaps which would cancel each other out and make the bird effectively float. This was fixed by adding a delay corresponding to the flight duration in which the action <em>flap</em> is nullified. One big problem found in the beginning was that the game rendered at 60FPS, which meant 60 actions each second. In early training, both actions are taken with similar probability, and in order to maintain an average stable $Y$, the agent needs to do nothing for about 95% of steps. This led to troubles with always hitting the top border. There was some thought put into modifying the action space to either flap or rest for $n$ steps. In the end I tackled this problem through reward shaping (which is covered below), but this hypothesis may also have led me to success.</p> <p>Regarding observation spaces, there was the choice between full and partial observability. Full observability would imply giving all available state information to the agent. This option has a greater performance ceiling, but comes with a ton of extra noise which would difficult training and require fancier image processing mechanisms such as convolutional autoencoders to reduce dimensionality and extract relevant features, and therefore reducing unnecessary noise. Here full observability would entail giving the game window, as well as other internal variables. Since this is a relatively simpler environment, I chose to stick with partial observability and hand-craft the features. The list of features I settled on were the following:</p> <ul> <li>Bird’s vertical position (y-coordinate)</li> <li>Next pipe’s horizontal distance (how far away)</li> <li>Next pipe’s gap vertical position (top/bottom of opening)</li> <li>Bird’s distance from top/bottom of screen (helps avoid boundaries)</li> </ul> <p>This kept the agent simple, and without any unnecessary information. The idea was to give only things that the agent needed to rely on to make a decision. Constants like the gap or width of a pipe pair, bird’s hitbox, bird’s x-coordinate are not needed as the agent can learn those internally.</p> <h1 id="reward-shaping">Reward Shaping</h1> <p>Perhaps the most interesting and challenging aspect of this project. Coming from <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">theorical literature</a>, I learned that giving intermediate rewards to an agent is not ideal as it incentivizes a certain policy we think is best, while there may be a better policy that we may not be capable of seeing. In classical learning RL problems, such as the maze problem, rewards are typically only given at the goal state. In real problems, this often leads to such a sparse reward and agents simply can not learn. As such, here is the reward design I chose:</p> <ul> <li>Small survival reward -&gt; $+0.01/step$ meaning $+6/s$;</li> <li>Big reward for clearing a pipe -&gt; $+300$;</li> <li>Big punishment for colliding with either pipe or top/bottom borders -&gt; $-500$;</li> <li>Exponential punishment for consecutive flaps -&gt; $2 + (n_{consecutiveFlaps}^2)*0.01$</li> </ul> <p>The punishment for consecutive flaps was a way of dealing with over flapping in the beginning as I previously mentioned. One issue I had initially was a completely wrong scale that disregarded framerate. I was giving a survival reward of $2$, which meant $120/s$. This thought the agent that surviving was more important than clearing pipes, so this agent did not get very far. Another problem I had was finding a correct scale for the rewards. These numbers I came with came from thin air. While it worked, I wonder what other ways are there to optimize reward scaling.</p> <h1 id="results---discussion">Results &amp; Discussion</h1> <p>Using Stable Baselines 3’s features, I saved TensorBoard logs every $10k$ steps. Here are both the mean reward and mean episode length across steps:</p> <figure> <picture> <img src="/assets/img/Flappy-Bird-SB3/reward.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Mean reward across training steps</figcaption> </figure> <figure> <picture> <img src="/assets/img/Flappy-Bird-SB3/ep_len.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Mean episode length across training steps</figcaption> </figure> <p>Both plots show how the agent learned well the environment. At around $700k$ and $1M$ steps there were some hiccups. For the demo I chose the model at $920k$ steps, as it seems to be the most stable. Nevertheless, there are still many improvements to be made. The agent achieves a score of $20-40$ every few runs but in between gets stuck in the first few pipes and does blunt mistakes. I am not sure why after going through the first, the agent seems to be much more stable. Additionally, whenever the agent fails, it is very often due to bumping its head on the corner of the up pipe when exiting.</p> <p>On the technical side, on future experiments, I would like to implement a way to start training from other trained models rather than from scratch.</p> <p>This environment can be made harder in many ways, be it smaller or variable gaps, varying or bigger horizontal velocity, less spacing between pipes…etc. However, I think this is a good breakpoint to move on to other more interesting things, such as messing with Robots again.</p>]]></content><author><name></name></author><category term="tooling,"/><category term="implementation"/><summary type="html"><![CDATA[In this post, I explored a toy example to learn Stable Baselines3 (SB3) and Gymnasium.]]></summary></entry><entry><title type="html">Bi-Weekly Advisor Meeting 18th of December 2025</title><link href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-3/" rel="alternate" type="text/html" title="Bi-Weekly Advisor Meeting 18th of December 2025"/><published>2025-12-18T15:12:00+00:00</published><updated>2025-12-18T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-3</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-3/"><![CDATA[<h1 id="contextsummary">Context/Summary</h1> <p>On my previous meeting with my advising team, I was reminded that I was leaving on key thematic out of my state-of-the-art review, which will come in later in my work: <strong>Meta-Learning</strong>. So far I had been looked into:</p> <ul> <li>Contact-rich relating to Reinforcement Learning (<a href="/blog/2025/Bi-Weekly-Advisor-Meeting-1/">Post</a>);</li> <li>Contact-rich Human-Robot Collaboration (<a href="/blog/2025/Bi-Weekly-Advisor-Meeting-2/">Post</a>); <ul> <li>Intent comprehension through contact interaction (<a href="https://www.researchgate.net/publication/362027689_A_review_on_interaction_control_for_contact_robots_through_intent_detection">Paper</a>)</li> </ul> </li> </ul> <p>From here I was missing <em>Meta-Learning</em> and <em>Model-Based Reinforcement Learning</em>.</p> <p>Regarding the latter, I have not read articles focused solely on <em>Model-Based RL</em> with <em>Contact-rich HRC</em>. I have however stumbled across a few paragraphs in other papers discussing <strong>Human-Modelling</strong>, which will be something I will be elaborating further down in this post. Come to think of it, the human is the most complex and hard entity to model in the environments I am interested in.</p> <p>Regarding the former, these weeks have been a bumpy ride. <em>TLDR</em> is that I’m feeling big gaps in Meta-Learning theory to properly understand what I am reading. After tweaking the keywords, I settled on the following search equation:</p> <div class="language-vb highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TITLE</span><span class="o">-</span><span class="n">ABS</span><span class="o">-</span><span class="n">KEY</span> <span class="p">(</span> <span class="s">"human-robot interaction"</span> <span class="n">OR</span> <span class="s">"human robot interaction"</span> <span class="n">OR</span> <span class="s">"human-robot collaborat*"</span> <span class="n">OR</span> <span class="s">"human-robot cooperat*"</span> <span class="n">OR</span> <span class="s">"physical human-robot interaction"</span> <span class="n">OR</span> <span class="s">"phri"</span> <span class="n">OR</span> <span class="s">"collaborative robot*"</span> <span class="n">OR</span> <span class="s">"cobot*"</span> <span class="n">OR</span> <span class="s">"hrc"</span> <span class="p">)</span>
<span class="n">AND</span> <span class="n">TITLE</span><span class="o">-</span><span class="n">ABS</span><span class="o">-</span><span class="n">KEY</span> <span class="p">(</span> <span class="s">"meta RL"</span> <span class="n">OR</span> <span class="s">"meta-RL"</span> <span class="n">OR</span> <span class="s">"meta reinforce*"</span> <span class="n">OR</span> <span class="s">"meta-reinforce*"</span> <span class="n">OR</span> <span class="s">"meta learning"</span> <span class="n">OR</span> <span class="s">"meta-learning"</span> <span class="n">OR</span> <span class="s">"metalearning"</span> <span class="n">OR</span> <span class="s">"meta-learn*"</span> <span class="p">)</span>
</code></pre></div></div> <p>… that yielded 30 articles, from which I saved 12 with interesting titles: (bold means thoroughly analised)</p> <ul> <li><strong>Adaptable automation with modular deep reinforcement learning and policy transfer</strong></li> <li><strong>Complementary learning-team machines to enlighten and exploit human expertise</strong></li> <li><strong>Active Exploration and Parameterized Reinforcement Learning Applied to a Simulated Human-Robot Interaction Task</strong></li> <li><strong>A behavioural transformer for effective collaboration between a robot and a non-stationary human</strong></li> <li><em>Meta-Learning-Based Optimal Control for Soft Robotic Manipulators to Interact with Unknown Environments</em></li> <li><em>Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in Human-Robot Interaction</em></li> <li><em>Meta-Learning for Text-Based Object Localization in Robotic Manipulation with DNN and CLIP</em></li> <li><em>Concept of an Intuitive Human-Robot-Collaboration via Motion Tracking and Augmented Reality</em></li> <li><em>Fast User Adaptation for Human Motion Prediction in Physical Human–Robot Interaction</em></li> <li><em>Developing the robotic space-force boundary of physical interaction perception in an infant way</em></li> </ul> <p>Considering there are not review papers on this matter yet, I plan to do a short review of these papers in this post, highlighting the details they share, and also where I struggled more.</p> <p>Generally, I found myself a bit lost reading these papers. I feel like I still lack key foundations in Meta-Learning to properly grasp what I am reading. Given this, I have been considering to diverge my attention for a short period of time to read at least the few chapters of <em>A Tutorial on Meta-Reinforcement Learning</em> (DOI 10.1561/2200000080). This tutorial was published earlier in April of this year, on <em>Foundations and Trends in Machine Learning</em>. It is co-authored by <em>Chelsea Finn</em>, one of the biggest names in <em>Meta-Learning</em>. Simultaneously, I’m hesitant about possibly investing time into something that I will work with only much later on. It is easy to fall in the rabbit hole, so if I choose this path I have to thread carefully to only gather the key aspects to help me understand the papers I am trying to review for the review paper I plan to publish next year.</p> <p>On an unrelated note, I found a <a href="https://www.reddit.com/r/reinforcementlearning/comments/1pjrnrn/if_youre_learning_rl_i_wrote_a_tutorial_about/">post</a> about a very interesting <a href="https://www.reinforcementlearningpath.com/step-by-step-soft-actor-critic-sac-implementation-in-sb3-with-pytorch/">SAC tutorial</a> using SB3 and PyTorch while browsing through Reddit on <a href="https://www.reddit.com/r/reinforcementlearning">r/reinforcementlearning</a>. I am planning on following this soon, I reckon I will sort of kill 2 birds with 1 stone, as I have been meaning to start learning SB3 for my future endeavors as well as SAC, one of the leading algorithms in Reinforcement Learning <em>SoTA</em>. On the previous meeting, we talked about <em>Isaac Lab</em>, a very powerful RL simulator for robotics from NVIDIA. Since then, I left the learning of this simulator on hold. I feel like I benefit more from learning SB3 and such right now, as I can not do much more other than create simulation environments without knowing these tools first.</p> <h2 id="short-critical-review">Short Critical Review</h2> <p>I was initially thinking about doing a meta analysis of the papers read, but I did not gather enough quantitative data to do so. Given this, I will do more of a simple critical review, narrating the key themes and insights. I only analyzed the abstracts, introductions, related works and conclusions. Details about the methods in the individual studies are not important right now. For now, I’m content to find general trends and topics that pop up frequently in Meta-Learning studies.</p> <h4 id="a-behavioural-transformer-for-effective-collaboration-between-a-robot--and-a-non-stationary-human">A behavioural transformer for effective collaboration between a robot and a non-stationary human</h4> <p>This paper discusses a topic with which I was previously unfamiliar, but which is highly relevant: <strong>Human Modeling</strong></p> <p>In human-robot collaboration scenarios, the human is typically considered part of the environment. This makes the environment time-dependent, due to the non-stationary human behavior. The human can not be an agent because the robot has no direct control over its actions. If an environment is non-stationary it means that either or both the state transition function and the reward function vary over time.</p> <p>One frequent approach to deal with non-stationary environments is the use of Multiagent RL. Multiagent RL: Multiagent reinforcement learning (MARL) aims to tackle non-stationarity. This approach is not feasible for human-robot collaboration settings as it is not possible to control the human agents.</p> <details> <summary><b>Side Note on how MARL tackles non-stationarity</b> Click to expand</summary> <p> Answer from <a href="https://chatgpt.com/s/t_6940d38c62ac81919b8733de98e6b24a">GPT</a>, modified. </p> <p> In standard (single-agent) reinforcement learning, the environment is usually assumed to be <strong>stationary</strong>: the transition dynamics and reward function do not change over time. This assumption is violated as soon as <strong>multiple learning agents</strong> are present. </p> <h3>Why non-stationarity arises</h3> <p> In a multi-agent setting, each agent is part of the environment from the perspective of the others. As agents <strong>learn and update their policies</strong>, their behavior changes over time. Consequently: </p> <ul> <li> The <strong>transition dynamics</strong> experienced by any given agent change. </li> <li> The <strong>reward distribution</strong> may also change. </li> <li> From a single agent’s viewpoint, the environment is no longer stationary or Markovian. The Markovian property of an environment is the core idea that the future depends only on the present, not the past; the next state and reward are conditionally independent of all previous states and actions, given only the current state and action. </li> </ul> <p> This phenomenon is known as <strong>environmental non-stationarity</strong>. </p> <h3>Why MARL “aims to tackle” non-stationarity</h3> <p> Multiagent reinforcement learning explicitly acknowledges and addresses this issue by designing methods that can cope with or mitigate non-stationarity, for example: </p> <ul> <li> <strong>Centralized training, decentralized execution (CTDE):</strong><br/> During training, agents have access to joint observations or joint actions, making the learning problem closer to a stationary one. </li> <li> <strong>Opponent / agent modeling:</strong><br/> Agents learn representations or models of other agents’ behaviors, allowing them to adapt to policy changes. </li> <li> <strong>Joint policy learning:</strong><br/> Learning coordinated policies reduces uncontrolled policy drift. </li> <li> <strong>Game-theoretic formulations:</strong><br/> Equilibrium concepts (e.g., Nash equilibria) provide stable solution targets despite learning dynamics. </li> </ul> <h3>Key intuition (one-sentence takeaway)</h3> MARL aims to tackle non-stationarity because, in multi-agent environments, each agent’s learning continuously changes the environment experienced by others, violating the stationarity assumptions of standard RL. </details> <p>Given that <em>MARL</em> is not an option, this study also discusses some ways to model human behavior, among which:</p> <ul> <li>Programming rules (the article did not give examples about this)</li> <li>Imitation learning</li> <li>Probabilistic models with or without Boltzmann rationality (agent is <em>boundedly rational</em>: it tends to prefer higher-value actions, but it may still choose lower-value ones with some probability.)</li> </ul> <p>From this citations in article I also saved some potentially interesting articles on human modelling for AI applications:</p> <ul> <li><em>LESS is More: Rethinking Probabilistic Models of Human Behavior</em> (78 citations);</li> <li><em>Optimal Behavior Prior: Data-Efficient Human Models for Improved Human-AI Collaboration</em> (17 citations);</li> <li><em>Prediction of Human Behavior in Human–Robot Interaction Using Psychological Scales for Anxiety and Negative Attitudes Toward Robots</em> (588 citations)</li> </ul> <p>The article uses follows the <em>zero-shot Meta-Learning</em> paradigm. On the <em>Meta-Learning</em> tutorial aforementioned, there are chapters for both zero-shot and multi-shot Meta-Learning. As a general AI concept, I have the vague idea that a zero-shot approach is one where a model can understand a new sample never seen during training, relying on pre-learned semantic knowledge. This concept seems to overlap with the definition of <em>Meta-Learning</em>, but that is where my understanding ends. The search equation I mentioned in this article shows a <a href="https://www.researchgate.net/publication/369516237_Recent_advances_of_few-shot_learning_methods_and_applications">review article</a> about few-shot learning methods, but I failed to recognize before how it could relate to what I am studying. Might be worth a read.</p> <p>Like in other articles, a frequent environment used to test human models is the soccer pass. The agent has to learn the human pass behavior, more or less power, varying angles, etc.</p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_3/soccer.png" class="img-fluid rounded z-depth-1" width="800" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="active-exploration-and-parameterized-reinforcement--learning-applied-to-a-simulated-human-robot--interaction-task">Active exploration and parameterized reinforcement learning applied to a simulated human-robot interaction task</h4> <p>This study was too overly focused on multi-armed bandits. Multi-armed bandits are a typical type of problem used when one is first learning <em>Reinforcement Learning</em>. Whereas typical RL environments are modelled with a <em>Markov Decision Process</em> (MDP), involving state transition probability functions, discount factors, etc… bandits are a simpler environment with only one state. For example choosing a restaurant to eat for a series of days. The concern is shifted from which action on a certain state to only which action has a higher value.</p> <p>Like the previous study, this one also refers <em>Boltzmann Rationality</em> for modelling human actions as well as the 2D Soccer Pass environment. Although the latter only briefly.</p> <h4 id="adaptable-automation-with-modular-deep-reinforcement-learning-and-policy-transfer">Adaptable automation with modular deep reinforcement learning and policy transfer</h4> <p>This study talks about a lot of new (to me) ideas in modular Reinforcement Learning. In its core, Modular RL is the idea of decomposing a task into several simpler modules, which can be learned separately and reused in learning new manipulation tasks. Each module can have its own reward function.</p> <p>The general idea is built on the assumption that modules are reusable neural network functions. Therefore, they can be pre-trained and recombined in different ways to tackle new tasks. Thus, instead of training a single network on a complex task that would take a long time, one can train on many different task module networks.</p> <p>The paper proposes a new variation of the SAC algorithm, HASAC. Its goal is to enhance an agent’s adaptability to new tasks by transferring the learned policies of former tasks to the new task through a “hyper-actor”. I did not dive deeply into how this works, as I have still to properly study the baseline SAC algorithm.</p> <p>Uses the Meta World benchmark, from 2020, containing various single and multi-tasks robotic manipulation tasks.</p> <p>The study also highlights one concept that has a particular interest in Meta-Learning, inductive bias.</p> <p>From <a href="https://www.reddit.com/r/MLQuestions/comments/egof3l/comment/fc8u0fi/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">here</a>:</p> <blockquote> <p>Inductive bias is restricting your hypothesis space to a certain class of hypotheses. In simpler terms, it’s a guess about what the function looks like, before looking at data. Take regression: if you have a finite set of points, you can draw infinitely many different functions that fit them. But if you assume that the function is linear, you remove almost all possibilities (there are still infinitely many linear functions, but way, way fewer than functions in general). Common inductive biases are smoothness and sparsity. Pretty much all used models include smoothness somehow: similar inputs have similar outputs. Sparsity is also very common: it’s the assumption that the function depends on few inputs. Without an inductive bias, machine learning is impossible. If something in machine learning works well, more often than not it’s because of its inductive bias. For example, convolutional neural networks are biased towards the kind of structure present in images: pixels next to each other are similar, there are similar features (e.g. eyes) present in different locations, big features are made of smaller features. A consequence of this is the no-free-lunch theorem. If your inductive bias works well on some type of data, it has to work badly on other kinds of data.</p> </blockquote> <p>I think this takes special importance in Meta-Learning because the assumptions we make about the tasks we are learning can influence what we expect new tasks to look like. This may or may not be ideal. I would guess it depends if we know which tasks the agent might be faced with.</p>]]></content><author><name></name></author><category term="bi-weekly"/><summary type="html"><![CDATA[Context/Summary]]></summary></entry><entry><title type="html">Bi-Weekly Advisor Meeting 27 of November 2025</title><link href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2/" rel="alternate" type="text/html" title="Bi-Weekly Advisor Meeting 27 of November 2025"/><published>2025-11-27T15:12:00+00:00</published><updated>2025-11-27T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2/"><![CDATA[<h1 id="context">Context</h1> <ul> <li>Finished RL course <ul> <li>Use of Neural Networks for feature construction</li> <li>Policy Gradient Algorithms <ul> <li>Actor-Critic Methods</li> <li>Gaussian Policies for Continuous Action Spaces</li> </ul> </li> </ul> </li> <li>Search for papers combining HRC &amp; Contact-rich &amp; RL <ul> <li>No review papers found</li> <li>Broadening to non-review articles showed 124 articles under SE</li> <li>Should look for HRC &amp; Contact-Rich only, leaving RL out of it, maybe I’ll find a decent review paper this way</li> <li>Analysis of <em>Reinforcement Learning Based Variable Impedance Control for High Precision Human-robot Collaboration Tasks</em></li> </ul> </li> <li>Setting up containerized Isaac Lab <ul> <li>Settled on using Distrobox for ease of use and integration with host OS. Discarded Docker as I am not looking for isolation/sandboxing, I just need to use a software compatible with Ubuntu 22.04 (tested in Arch, launched but yielded many errors).</li> <li>Set up <a href="https://github.com/brunofavs/dotfiles/tree/main/scripts/Scripts/installation_scripts">installation scripts</a> to get all my dev tools, Isaac and Cuda working with one command. This will be useful whenever I need to work on one of the lab’s server. And also for the sake of reproducibility if I ever need to reinstall. Or even if somebody else in the lab wants to try it out.</li> <li>Isaac requires Ubuntu 22.04 and Python 3.11. To ensure Python compatibility I used <code class="language-plaintext highlighter-rouge">pyenv</code> with the <code class="language-plaintext highlighter-rouge">pyenv-virtualenv</code> plugin. Up until now I just used <code class="language-plaintext highlighter-rouge">virtualenvwrapper</code>, but this tool does not allow the creation of virtual environments with different Python versions.</li> </ul> </li> </ul> <h1 id="reinforcement-learning-based-variable-impedance-control-for-high-precision-human-robot-collaboration-tasks"><strong>Reinforcement Learning Based Variable Impedance Control for High Precision Human-robot Collaboration Tasks</strong></h1> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_2/paper_img1.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="descriptive-analysis">Descriptive Analysis</h3> <p>This paper proposed an impedance control method for high precision collaborative assembly tasks. The case study involved fitting shafts through cuboids with holes. Classic Peg-in-Hole. The shaft-hole clearance was 0.1 mm. The authors experimented both with 1 or 2 holes/shafts. The cuboid initial position was fixed and the shafts’ initial positions varied within a bounded range.</p> <p>In the first phase, using a depth camera, the robot locates and grabs the cuboid, and then moves along a reference trajectory without the human operator’s intervention. In the second phase, the human operator needs to correct the possible position deviation. During this time, the robot follows the human operator’s intention and senses the human force to change the end effector’s position.</p> <p>This work used PPO to determine the impedance parameters <em>M</em>, <em>B</em>, <em>K</em> (inertia, damping, and stiffness). After, these parameters were fed to the robots’ controller, which uses the following differential equation to compute the change in position.</p> \[M_t \Delta \ddot{x} + B_t \Delta \dot{x} + K_t \Delta x = \Delta f\] <p>The reward signal was the sum of the distance between the two end points of the object and the two target points.</p> <p>In simulation, the human force was replicated by a motor applying a force proportional to the distance from the target position. The authors argued that this approach has similar reasoning in comparison to the forces applied by a human. A human would apply a force proportional to how far off the shaft was from the hole.</p> <p>The authors reported that due to the limitations of the physics engine in the simulation environment, the cuboid may get stuck due to collision detection. The simulator used was MuJoCo.</p> <h3 id="key-takeaways-for-my-work">Key Takeaways for My Work</h3> <p>While this article presents interesting information, it diverges from my work in a few ways and has some notable gaps:</p> <ul> <li>I’m not so sure that this work is not a pure <em>contact-rich</em> task <em>per se</em>. It covers pre-contact, which I’m not interested in. It is not so clear through the paper how they get to the change in force used for in the aforementioned equation. I reckon they are just using the net force applied on the end effector. They do not seem to be taking the contact data to make the agent smarter. The human is the master and the robot is the slave, following blindly what the human suggests. The RL agent “simply” learns the impedance parameters so that the approach would still work for many humans applying different forces.</li> <li>While they show a physical set-up in the beginning to draw readers in, they later reveal that they did not leave simulation.</li> <li>The paper regards more the precision of the collaboration, whereas I would like my research to steer more into allowing a single agent to collaborate in multiple tasks, hence the later objective of implementing Meta RL.</li> </ul> <p>The papers highlight some points that are indeed relevant though:</p> <ul> <li>Differences in the skills of various operators bring difficulties to collaborative robots. This raises an extremely important question. Should I tailor the agent I will build to be sort of <strong>my</strong> little metal friend, designed to collaborate efficiently <strong>WITH ME</strong>, or be designed to collaborate with anyone? I believe both options are valid, and the former is definitely easier to train and has potential for more interesting results, which can be eye-catchy in an article.</li> <li>Common collaboration tasks are collaborative assembly and collaborative transportation, with a special emphasis on the latter with heavier objects.</li> </ul>]]></content><author><name></name></author><category term="bi-weekly"/><summary type="html"><![CDATA[Context]]></summary></entry><entry><title type="html">Why does the Policy Improvement Theorem not hold true in function approximation</title><link href="https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx/" rel="alternate" type="text/html" title="Why does the Policy Improvement Theorem not hold true in function approximation"/><published>2025-11-21T15:12:00+00:00</published><updated>2025-11-21T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx/"><![CDATA[<p><em>Page 254 Sutton</em></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"*The root cause of the difficulties with the discounted control setting is that with
function approximation we have lost the policy improvement theorem (Section 4.2). It is
no longer true that if we change the policy to improve the discounted value of one state
then we are guaranteed to have improved the overall policy in any useful sense. That
guarantee was key to the theory of our reinforcement learning control methods. With
function approximation we have lost it!*"
</code></pre></div></div> <p><a href="https://ai.stackexchange.com/questions/45071/why-policy-improvement-theorem-cant-be-applied-in-case-of-function-approximatio">Stack Exchange Useful Link</a></p> <p>Equation (4.7) from chapter <em>4.2</em> states that given $\pi$ and $\pi’$ as any pair of deterministic policies such that, for all $s \in \mathcal{S}$ :</p> \[q_{\pi}(s,\pi'(s)) \ge v_{\pi}(s)\] <p>Must mean that $\pi’$ is a strict improvement over $\pi$. A particular case occurs when all state values are equal but one. This was the basis of many tabular control methods. Tabular controls methods can improve individual states, so they allow <em>PIT</em> to be applied.</p> <h3 id="when-using-function-approximation">When using function approximation…</h3> <p>… any update in the weights modifies all states. Which means some get better, others get worse. It is a trade-off. Hence, the name function <strong>approximation</strong>. If we cannot assure that all state values are <em>at least</em> equal, the policy improvement theorem does not apply. Note that <em>PIT</em> also requires policies to be deterministic. Policies learned by Function approximation can both be deterministic or stochastic, so this particular condition can still be applied.</p> <h1 id="solution">Solution</h1> <p>Later in the same page…</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>*In Chapter 13 we introduce an
alternative class of reinforcement learning algorithms based on parameterized policies,
and there we have a theoretical guarantee called the “policy-gradient theorem” which
plays a similar role as the policy improvement theorem.*
</code></pre></div></div> <p>This is the class of algorithms I’m eager to learn about. These include all the Actor-Critic methods such as <strong>SAC</strong>. SAC is a state of the art algorithm used extensively in Robotics-RL research!</p>]]></content><author><name></name></author><category term="theory"/><summary type="html"><![CDATA[Page 254 Sutton]]></summary></entry><entry><title type="html">Bi-Weekly Advisor Meeting 06 of November 2025</title><link href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1/" rel="alternate" type="text/html" title="Bi-Weekly Advisor Meeting 06 of November 2025"/><published>2025-11-06T15:12:00+00:00</published><updated>2025-11-06T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1/"><![CDATA[<h1 id="context">Context</h1> <p>Last meeting, on the 24th of October, we concluded that it would be better to set bi-weekly deadlines to ensure that I didn’t wander off too much in the vast world of RL and Robotics and could ensure progress on my PhD problem. With this in mind, we agreed that it would be good to analyze some review papers on the fields of:</p> <ul> <li>Contact-Rich Robotics;</li> <li>Deep Reinforcement Learning;</li> <li>Model-based Reinforcement Learning.</li> </ul> <p>I began my search on Scopus with the following search equation: <code class="language-plaintext highlighter-rouge">TITLE ( "Robo*" ) AND ( TITLE ( "contact" ) OR TITLE-ABS-KEY ( "touch" ) ) AND PUBYEAR &gt; 2022 AND PUBYEAR &lt; 2026 AND ( LIMIT-TO ( DOCTYPE , "re" ) )</code></p> <p>Which yielded 34 results. Amongst those, a particular one caught my eye, named <strong>“A review on reinforcement learning for contact-rich robotic manipulation tasks”</strong> (<a href="https://doi.org/10.1016/j.rcim.2022.102517"><strong>DOI</strong></a>). This paper has 159 citations according to Google Scholar and was published on <em>Robotics and Computer-Integrated Manufacturing</em> (IF:11.4).</p> <details> <summary><b>PDF</b> Click to expand</summary> <object data="/assets/pdf/Elguea-Aguinaco_et_al._-_2023_-_A_review_on_reinforcement_learning_for_contact-rich_robotic_manipulation_tasks.pdf" width="1000" height="1000" type="application/pdf"></object> </details> <p>In sum, the paper touched on relevant topics such as:</p> <ul> <li>Rigid vs Deformable Object Manipulation;</li> <li>Tasks usually performed for both types of objects;</li> <li>Reinforcement Learning Algorithms most commonly used;</li> </ul> <h1 id="paper-analysis">Paper Analysis</h1> <h2 id="general-points">General Points</h2> <p><br/> Throughout the paper, an emphasis was put on contact stability. Many works dealt with issue due to contact stability.</p> <p>About 35% of the papers reviewed rely on human demonstration-assisted learning.</p> <h2 id="rigid-object-manipulation">Rigid Object Manipulation</h2> <p><br/></p> <p>The most common task covered by the papers reviewed was <em>Assembly and Insertion tasks</em>. <strong>Peg in Hole</strong> tasks are the most common benchmark as the problems that arise with this task are commonly found in assemblies. Performance, sample-efficiency and generalization capabilities are often the 3 desired goals. When it comes to performance, studies measure their performance targeted towards different purposes. Amongst others, precision, stability, safety, execution time or robustness.</p> <p>Safety wise, authors constrained the robot’s displacement and velocity motion commands, as well as the contact forces on the end-effector. In RL terms, the action space was constrained, which trades safety for prohibiting exploration leading to potentially better policies.</p> <p>Long training periods was also one of the reported issues. This is a recurrent drawback in high-dimensional tasks where large exploration is required. Therefore, many studies also put their efforts into pursuing greater sample efficiency. Many studies opted to find ways to give initial knowledge to the agents through various methods. For example: Human demonstrations; gathering knowledge from assembly tasks to use in disassembly tasks; combining RL with Fuzzy Logic using more hand-engineered rewards, etc.</p> <p>On the other end, the pursuit of sample efficiency generally leads to worse generalization capabilities, which in turn also worsens the agent transfer from simulation to reality. In this sense, a widely used approach in multiple studies is domain randomization. This technique allows to uniformly randomize a distribution of real data in predefined ranges in each training episode to obtain more robust policies. Two broad methods of domain randomization may be distinguished: visual randomization and dynamic randomization. Visual randomization targets to provide sufficient simulated variability of visual parameters, such as object location and their estimation in space or illumination. Dynamic randomization can help to acquire a robust control policy by randomizing various physical parameters in the simulator such as object dimensions, friction coefficients, or the damping coefficients of the robot joints, among others.</p> <p>Other studies in the pursuit of sample efficiency and generalization focus their attention on reward shaping and using models. There are studies using inverse RL from human demonstrations to learn a reward function. More recent studies combined dynamic movement primitives (DMPs) with RL to improve generalization. DMPs allow the contact-rich task to be divided into low-dimensional, easily adaptable sub-tasks, which also increase training efficiency. Others for instance, used meta-RL. Meta-learning enhances sample efficiency, as it enables explicit learning of a latent structure over a family of tasks that can later be easily adapted to the target task.</p> <p>In addition to assembly tasks, these tasks and associated challenges are also explored in studies:</p> <ul> <li>Disassembly; <ul> <li>High variability between items;</li> </ul> </li> <li>Polishing and Grinding; <ul> <li>Contact-stability is imperative;</li> </ul> </li> <li>Stacking and Unstacking; <ul> <li>Different from Pick &amp; Place because there is object to object contact;</li> </ul> </li> <li>Door/Drawer Opening; <ul> <li>Finding the correct axis of movement;</li> </ul> </li> <li>Pushing; <ul> <li>Unknown surface friction properties;</li> </ul> </li> <li>Mixing/Scooping;</li> <li><strong>Multiple tasks</strong> <ul> <li>High need for generalization.</li> </ul> </li> </ul> <h2 id="deformable-object-manipulation">Deformable Object Manipulation</h2> <p><br/></p> <p>Many of the problems presented before still hold here. For the moment I don’t plan on working with deformable objects so I will not elaborate further here. Nonetheless. there are the most frequent tasks:</p> <ul> <li>Rope Folding;</li> <li>Fabrics Folding;</li> <li>Tissue tensioning;</li> <li>Tissue cutting;</li> <li>Peg in hole (with one of parts being deformable).</li> </ul> <p>Here lies a distribution of tasks covered by studies for both Rigid and Deformable objects: </p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/5.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="comparison-of-algorithms-used">Comparison of algorithms used</h1> <p><br/></p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/6.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="safety-strategies-used">Safety strategies used</h1> <p><br/></p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/7.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="overview-of-studies-in-rigid-object-manipulation">Overview of studies in Rigid Object Manipulation</h1> <p><br/></p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/1.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/2.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/3.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="bi-weekly"/><summary type="html"><![CDATA[Context]]></summary></entry></feed>