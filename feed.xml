<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://brunofavs.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://brunofavs.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-24T22:17:08+00:00</updated><id>https://brunofavs.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Bi-Weekly Advisor Meeting 27 of November 2025</title><link href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2/" rel="alternate" type="text/html" title="Bi-Weekly Advisor Meeting 27 of November 2025"/><published>2025-11-27T15:12:00+00:00</published><updated>2025-11-27T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-2/"><![CDATA[<h1 id="draft">Draft</h1> <h2 id="context">Context</h2> <ul> <li>Finished RL course <ul> <li>Use of Neural Networks for feature construction</li> <li>Policy Gradient Algorithms <ul> <li>Actor-Critic Methods</li> <li>Gaussian Policies for Continuous Action Spaces</li> </ul> </li> </ul> </li> <li>Search for papers combining HRC &amp; Contact-rich &amp; RL <ul> <li>No review papers found</li> <li>Broadening to non-review articles showed 124 articles under SE</li> <li>Should look for HRC &amp; Contact-Rich only, leaving RL out of it, maybe I’ll find a decent review paper this way</li> <li>Analysis of <em>Reinforcement Learning Based Variable Impedance Control for High Precision Human-robot Collaboration Tasks</em></li> </ul> </li> <li>Setting up containerized Isaac Lab <ul> <li>Settled on using Distrobox for ease of use and integration with host OS. Discarded Docker as I am not looking for isolation/sandboxing, I just need to use a software compatible with Ubuntu 22.04 (tested in Arch, launched but yielded many errors).</li> <li>Set up <a href="https://github.com/brunofavs/dotfiles/tree/main/scripts/Scripts/installation_scripts">installation scripts</a> to get all my dev tools, Isaac and Cuda working with one command. This will be useful whenever I need to work on one of the lab’s server. And also for the sake of reproducibility if I ever need to reinstall.</li> <li>Isaac requires Ubuntu 22.04 and Python 3.11. To ensure Python compatibility I used <code class="language-plaintext highlighter-rouge">pyenv</code> with the <code class="language-plaintext highlighter-rouge">pyenv-virtualenv</code> plugin. Up until now I just used <code class="language-plaintext highlighter-rouge">virtualenvwrapper</code>, but this tool does not allow the creation of virtual environments with different Python versions.</li> </ul> <p>There is a mistake in this sentence</p> </li> </ul>]]></content><author><name></name></author><category term="bi-weekly"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">Why does the Policy Improvement Theorem not hold true in function approximation</title><link href="https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx/" rel="alternate" type="text/html" title="Why does the Policy Improvement Theorem not hold true in function approximation"/><published>2025-11-21T15:12:00+00:00</published><updated>2025-11-21T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/PTI_in_Func_Approx/"><![CDATA[<p><em>Page 254 Sutton</em></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"*The root cause of the difficulties with the discounted control setting is that with
function approximation we have lost the policy improvement theorem (Section 4.2). It is
no longer true that if we change the policy to improve the discounted value of one state
then we are guaranteed to have improved the overall policy in any useful sense. That
guarantee was key to the theory of our reinforcement learning control methods. With
function approximation we have lost it!*"
</code></pre></div></div> <p><a href="https://ai.stackexchange.com/questions/45071/why-policy-improvement-theorem-cant-be-applied-in-case-of-function-approximatio">Stack Exchange Useful Link</a></p> <p>Equation (4.7) from chapter <em>4.2</em> states that given $\pi$ and $\pi’$ as any pair of deterministic policies such that, for all $s \in \mathcal{S}$ :</p> \[q_{\pi}(s,\pi'(s)) \ge v_{\pi}(s)\] <p>Must mean that $\pi’$ is a strict improvement over $\pi$. A particular case occurs when all state values are equal but one. This was the basis of many tabular control methods. Tabular controls methods can improve individual states, so they allow <em>PIT</em> to be applied.</p> <h3 id="when-using-function-approximation">When using function approximation…</h3> <p>… any update in the weights modifies all states. Which means some get better, others get worse. It is a trade-off. Hence, the name function <strong>approximation</strong>. If we cannot assure that all state values are <em>at least</em> equal, the policy improvement theorem does not apply. Note that <em>PIT</em> also requires policies to be deterministic. Policies learned by Function approximation can both be deterministic or stochastic, so this particular condition can still be applied.</p> <h1 id="solution">Solution</h1> <p>Later in the same page…</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>*In Chapter 13 we introduce an
alternative class of reinforcement learning algorithms based on parameterized policies,
and there we have a theoretical guarantee called the “policy-gradient theorem” which
plays a similar role as the policy improvement theorem.*
</code></pre></div></div> <p>This is the class of algorithms I’m eager to learn about. These include all the Actor-Critic methods such as <strong>SAC</strong>. SAC is a state of the art algorithm used extensively in Robotics-RL research!</p>]]></content><author><name></name></author><category term="theory"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry><entry><title type="html">Bi-Weekly Advisor Meeting 06 of November 2025</title><link href="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1/" rel="alternate" type="text/html" title="Bi-Weekly Advisor Meeting 06 of November 2025"/><published>2025-11-06T15:12:00+00:00</published><updated>2025-11-06T15:12:00+00:00</updated><id>https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1</id><content type="html" xml:base="https://brunofavs.github.io/blog/2025/Bi-Weekly-Advisor-Meeting-1/"><![CDATA[<h1 id="context">Context</h1> <p>Last meeting, on the 24th of October, we concluded that it would be better to set bi-weekly deadlines to ensure that I didn’t wander off too much in the vast world of RL and Robotics and could ensure progress on my PhD problem. With this in mind, we agreed that it would be good to analyze some review papers on the fields of:</p> <ul> <li>Contact-Rich Robotics;</li> <li>Deep Reinforcement Learning;</li> <li>Model-based Reinforcement Learning.</li> </ul> <p>I began my search on Scopus with the following search equation: <code class="language-plaintext highlighter-rouge">TITLE ( "Robo*" ) AND ( TITLE ( "contact" ) OR TITLE-ABS-KEY ( "touch" ) ) AND PUBYEAR &gt; 2022 AND PUBYEAR &lt; 2026 AND ( LIMIT-TO ( DOCTYPE , "re" ) )</code></p> <p>Which yielded 34 results. Amongst those, a particular one caught my eye, named <strong>“A review on reinforcement learning for contact-rich robotic manipulation tasks”</strong> (<a href="https://doi.org/10.1016/j.rcim.2022.102517"><strong>DOI</strong></a>). This paper has 159 citations according to Google Scholar and was published on <em>Robotics and Computer-Integrated Manufacturing</em> (IF:11.4).</p> <details> <summary><b>PDF</b> Click to expand</summary> <object data="/assets/pdf/Elguea-Aguinaco_et_al._-_2023_-_A_review_on_reinforcement_learning_for_contact-rich_robotic_manipulation_tasks.pdf" width="1000" height="1000" type="application/pdf"></object> </details> <p>In sum, the paper touched on relevant topics such as:</p> <ul> <li>Rigid vs Deformable Object Manipulation;</li> <li>Tasks usually performed for both types of objects;</li> <li>Reinforcement Learning Algorithms most commonly used;</li> </ul> <h1 id="paper-analysis">Paper Analysis</h1> <h2 id="general-points">General Points</h2> <p><br/> Throughout the paper, an emphasis was put on contact stability. Many works dealt with issue due to contact stability.</p> <p>About 35% of the papers reviewed rely on human demonstration-assisted learning.</p> <h2 id="rigid-object-manipulation">Rigid Object Manipulation</h2> <p><br/></p> <p>The most common task covered by the papers reviewed was <em>Assembly and Insertion tasks</em>. <strong>Peg in Hole</strong> tasks are the most common benchmark as the problems that arise with this task are commonly found in assemblies. Performance, sample-efficiency and generalization capabilities are often the 3 desired goals. When it comes to performance, studies measure their performance targeted towards different purposes. Amongst others, precision, stability, safety, execution time or robustness.</p> <p>Safety wise, authors constrained the robot’s displacement and velocity motion commands, as well as the contact forces on the end-effector. In RL terms, the action space was constrained, which trades safety for prohibiting exploration leading to potentially better policies.</p> <p>Long training periods was also one of the reported issues. This is a recurrent drawback in high-dimensional tasks where large exploration is required. Therefore, many studies also put their efforts into pursuing greater sample efficiency. Many studies opted to find ways to give initial knowledge to the agents through various methods. For example: Human demonstrations; gathering knowledge from assembly tasks to use in disassembly tasks; combining RL with Fuzzy Logic using more hand-engineered rewards, etc.</p> <p>On the other end, the pursuit of sample efficiency generally leads to worse generalization capabilities, which in turn also worsens the agent transfer from simulation to reality. In this sense, a widely used approach in multiple studies is domain randomization. This technique allows to uniformly randomize a distribution of real data in predefined ranges in each training episode to obtain more robust policies. Two broad methods of domain randomization may be distinguished: visual randomization and dynamic randomization. Visual randomization targets to provide sufficient simulated variability of visual parameters, such as object location and their estimation in space or illumination. Dynamic randomization can help to acquire a robust control policy by randomizing various physical parameters in the simulator such as object dimensions, friction coefficients, or the damping coefficients of the robot joints, among others.</p> <p>Other studies in the pursuit of sample efficiency and generalization focus their attention on reward shaping and using models. There are studies using inverse RL from human demonstrations to learn a reward function. More recent studies combined dynamic movement primitives (DMPs) with RL to improve generalization. DMPs allow the contact-rich task to be divided into low-dimensional, easily adaptable sub-tasks, which also increase training efficiency. Others for instance, used meta-RL. Meta-learning enhances sample efficiency, as it enables explicit learning of a latent structure over a family of tasks that can later be easily adapted to the target task.</p> <p>In addition to assembly tasks, these tasks and associated challenges are also explored in studies:</p> <ul> <li>Disassembly; <ul> <li>High variability between items;</li> </ul> </li> <li>Polishing and Grinding; <ul> <li>Contact-stability is imperative;</li> </ul> </li> <li>Stacking and Unstacking; <ul> <li>Different from Pick &amp; Place because there is object to object contact;</li> </ul> </li> <li>Door/Drawer Opening; <ul> <li>Finding the correct axis of movement;</li> </ul> </li> <li>Pushing; <ul> <li>Unknown surface friction properties;</li> </ul> </li> <li>Mixing/Scooping;</li> <li><strong>Multiple tasks</strong> <ul> <li>High need for generalization.</li> </ul> </li> </ul> <h2 id="deformable-object-manipulation">Deformable Object Manipulation</h2> <p><br/></p> <p>Many of the problems presented before still hold here. For the moment I don’t plan on working with deformable objects so I will not elaborate further here. Nonetheless. there are the most frequent tasks:</p> <ul> <li>Rope Folding;</li> <li>Fabrics Folding;</li> <li>Tissue tensioning;</li> <li>Tissue cutting;</li> <li>Peg in hole (with one of parts being deformable).</li> </ul> <p>Here lies a distribution of tasks covered by studies for both Rigid and Deformable objects: </p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/5.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="comparison-of-algorithms-used">Comparison of algorithms used</h1> <p><br/></p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/6.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="safety-strategies-used">Safety strategies used</h1> <p><br/></p> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/7.review_paper.png" class="img-fluid rounded z-depth-1" width="1000" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="overview-of-studies-in-rigid-object-manipulation">Overview of studies in Rigid Object Manipulation</h1> <p><br/></p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/1.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/2.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <img src="/assets/img/bi_weekly_advisor_meeting_1/3.review_paper.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="bi-weekly"/><summary type="html"><![CDATA[this is what included images could look like]]></summary></entry></feed>